
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
STAT 374, Homework 4 \hfill Aaron Maurer
\vspace{2mm}
\hrule
\vspace{2mm}
\begin{itemize} 
    \item[1.]
        \begin{itemize} 
            \item[(a)] Averaging over $100$ simulations where $500$ points
were drawn from a normal means model with each mean equal to $2$, the average
risk of th mean was $1.016$, and the average risk of the isotonic regression
was $6.159$. As one might expect, since the MLE makes a stronger and correct
assumption, it has lower risk. However, the monotone regression still has
fairly low risk.
            \item[(b)] I estimated the risk for every fourth $n$ betwen 20 and
200. For each, I averged over 500 simulations. It seems that the risks are
proportional; the regression on the linear function seems to have about $25\%$
less risk than the step function for each $n$, with both risks growing at a
slower rate than linear, possibly proportional to \(\frac{log(n)}{n}\).
                \begin{center}
                    \includegraphics[width=9cm]{hw4/1_b} 
                \end{center}


            \item[(c)] Let $c(x_n)$ be the concave sequence of points evaluated at points $x_n$, and let $s(x_n)$ be a corresponding convex sequence. I will attempt to show that if $s(x_n)$ is not linear, it is superseded by some linear function. We can break this problem down into three cases.
                \begin{itemize}
                    \item[case 1:] If $s(x_n)>c(x_n)$ for all $x$, then there is trivially a better fit $s(x_n)-c$, where $c$ is the minimum distance between the two sequences. This improved fit is of the next case, so if there is a superseding linear fit for it, there is one for this case as well.
                    \item[case 2:] $s(x_n)=c(x_n)$ for at least one $n$, but it is never the case $s(x_n)<c(x_n)$. Let $s(x_1)=c(x_1)$. Since $s$ and $c$ are respectively convex and concave, there exists a line passing through $s(x_1)=c(x_1)$ such that $s(x_n)\geq l(x_n) \geq c(x_n) \, \forall n$. If $s(x_n)$ isn't itself a line, then there will be a $n$ for which their is strict inequality on the left hand side, making $l$ a superior fit to $s$.
                    \item[case 3:] $s(x_n)<c(x_n)$ for some $n$. Let us flesh out $s(x_n)$ and $c(x_n)$, such that $s(x)$ and $c(x)$ are piecewise linear functions connecting the points of each sequence. These functions are necessarily also convex and concave respectively. We once again have three cases:
                        \begin{itemize}
                            \item[case 3.1:] If $s(x)$ and $c(x)$ never intersect, a shift as in case 1 will achieve a superior fit and reduce the problem to the next case.
                            \item[case 3.2:] If $s(x)$ and $c(x)$ intersect exactly once, we can replace $s(x_n)$ with $s(x_n)+c$ for $n$ such that $s(x_n)<c(x_n)$, where $c$ is the minimum distance between the sequences on this set. This action will preserve convexity while creating an improved fit (the distance is strictly reduced), and reduces this case to the next case. 
                            \item[case 3.3:] If they $s(x)$ and $c(x)$ intersect at multiple points, we can choose two intersection points $a<b$. Let $l$ be the line passing through them. It is a secant line for both $s$ and $c$, so by their respective convexity/concavity, $c(x)\geq l(x) \geq s(x)$ on $[a,b]$ and $c(x)\leq l(x) \leq s(x)$ elsewhere. Thus, if $c(x)\neq l(x)$ for some $x$, then $l$ is a superior fit to the function $s$. Since $s$ and $c$ are linear between the $x_n$, this can only arise if $s(x_n)\neq l(x_n)$ for some $n$, making $l(x_n)$ a superior fit to $c(x_n)$.

                        \end{itemize}
                \end{itemize}
        \end{itemize}
    \item[2.]
        I have computed the mean squared error for each of the methods:
        \FloatBarrier
        \input{hw4_2} 
        \FloatBarrier
        \begin{itemize}
            \item[(a)]
                At a low level of noise, the wavelet methods all preform better than the local linear model. With low noise, their ability to accurately match the fine features of the Doppler function win out. None of them preform terribly though. On the other hand, once the noise increases, the James Stein and Universal Threshold methods get worse much quicker than Sure Shrink and local linear, which are comparatively much better. This is likely since it becomes harder to distinguish feature from noise. In both cases, the James Stein is worse than the Universal Threshold, and Local Linear loses out to Sure Shrink. As the graphs note the distinction between local linear regression and Sure Shrink are hard to pick out by eye  \\
                \includegraphics[width=9cm]{hw4/2_ab_pt1} 
                \includegraphics[width=9cm]{hw4/2_ab_pt01} \\
            \item[(b)]
                On the other hand, once there are occasional spikes in the variance, the local linear model preforms the best. As the graph shows, it preforms more smoothing on the occasional spikes than the Sure Shrink, which likely has them identified as true features.
                \begin{center}
                    \includegraphics[width=9cm]{hw4/2_ab_bimodal} 
                \end{center}
        \end{itemize}
    \item[3.]
        \begin{itemize}
            \item[(a)]
                I have plotted out all of the fits in the graph below. The isotonic regression provides a good fit only over through day 25 or so, when the observed values cease to be monotone. After that, the shorter fits do a reasonable job over the middle part, which is essentially flat. However, the more data the regression is fit on where the trend actually seems to be decreasing, the worse the regression preforms. Assuming the error terms are symmetrically distributed, one would expect at worst $50\%$ of the observations to fall below the actual series. One could thus generate a test statistic based on how the number of times, as the function is generated step by step, the function is forced to drop in response to a lower observed value within a particular range. This statistic would be approximately binomially distributed (though there is additional uncertainty due to the function being noisy), and a critical value could be generated from this to determine whether the range isn't monotone.
                \begin{center}
                    \includegraphics[width=9cm]{hw4/3_a} 
                \end{center}
            \item[(b)] 
                The series of local linear fits are generally pretty good. However, as they are fit in increasingly noisy parts of the data, they have a greater amount of uncertainty, since the confidence interval is fit with a constant variance assumption.
                    \includegraphics[width=9cm]{hw4/3_b25day} 
                    \includegraphics[width=9cm]{hw4/3_b50day}  \\
                    \includegraphics[width=9cm]{hw4/3_b75day} 
                    \includegraphics[width=9cm]{hw4/3_b85day}  \\
                    \includegraphics[width=9cm]{hw4/3_b171day} 
            \item[(c)]
                The Haar wavelet fit is below. Its fit is comparable on the relatively smooth portion of the series. However, in the noisy section to the end, it mostly clings to the large fluctuations in the data, while the others averaged them out in one way or another. Since these fluctuations are due to the small number of surviving flies, rather than a reflection of the true rate, this constitutes a worse fit.
                \begin{center}
                    \includegraphics[width=9cm]{hw4/3_c} 
                \end{center}
                {\textbf My Code:}
               \lstinputlisting[firstline=181,lastline=210]{hw4.R}
            \item[(d)]
                The Daubechies wavelet fit, much like the Haar, clings to closely to the observed data when the noise accounts for large swings, but otherwise has a similar, somewhat smoother, fit over the relatively calm part of the data.
                \begin{center}
                    \includegraphics[width=9cm]{hw4/3_d} 
                \end{center}
            
        \end{itemize}
    \item[4.]
        \begin{itemize}
            \item[(a)]
                \begin{align*}
                    P(\mu \vert \mathcal{D}_n) &= \frac{\mathcal{L}_n(\mu) \pi(\mu)}{\int_{\R^d}\mathcal{L}_n(\mu) \pi(\mu) \, d\mu} \\
                    P(\mu \vert \mathcal{D}_n) &= \frac{\prod_{i=1}^n \frac{1}{\sqrt{(2\pi)^d}}\exp\left(-\frac{1}{2}\|x_i-\mu\|^2\right)}{\int_{\R^d}\prod_{i=1}^n\frac{1}{\sqrt{(2\pi)^d}} \exp\left(-\frac{1}{2}\|x_i-\mu\|^2\right) \, d\mu}  \\
                    P(\mu \vert \mathcal{D}_n) &= \frac{ \exp\left(-\frac{1}{2}\sum_{i=1}^n\|x_i-\mu\|^2\right)}{\int_{\R^d}\exp\left(-\frac{1}{2}\sum_{i=1}^n\|x_i-\mu\|^2\right) \, d\mu}  \\
                    P(\mu \vert \mathcal{D}_n) &= \frac{ \exp\left(-\frac{n}{2}\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2 -\frac{n}{2}\left(\sum_{i=1}^n\frac{\|x_i\|^2}{n}-\left\|\sum_{i=1}^n\frac{x_i}{n}\right\|^2\right)\right)}{\int_{\R^d}\exp\left(-\frac{n}{2}\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2 -\frac{n}{2}\left(\sum_{i=1}^n\frac{\|x_i\|^2}{n}-\left\|\sum_{i=1}^n\frac{x_i}{n}\right\|^2\right)\right) \, d\mu}  \\
                    P(\mu \vert \mathcal{D}_n) &= \frac{ \exp\left(-\frac{n}{2}\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2\right)  \, d\mu}{\int_{\R^d}\exp\left(-\frac{n}{2}\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2\right)}  \\
                    P(\mu \vert \mathcal{D}_n) &= \sqrt{\frac{n^d}{(2\pi)^d}} \exp\left(-\frac{n}{2}\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2\right) \\
                \end{align*}
                So, we can conclude that the posterior is distributed as \(N\left(\sum_{i=1}^n\frac{x_i}{n},\frac{1}{n}I_d\right)\).
        \item[(b)]
                By the previous result, the posterior for $\mu$ is normally distributed with a covariance matrix of $\frac{1}{n}I_d$. This implies that each $\mu_i$ is independent from the others (since they have covariance $0$ and the function is multivariate normal. Accordingly, if $Z\sim N(0,I_d)$, then \(\mu \sim \sum_{i=1}^n\frac{x_i}{n}+\frac{Z}{n}\). Thus, $\|n\mu\|^2=n\theta$ has a non-central $\chi^2$ distribution with $d$ degrees of freedom and noncentrality parameter \(\|\sum_{i=1}^n{x_i}\|^2\). This in turn means that $\theta\sim \frac{\chi^2}{n}$ distribution with the same offset parameter and degrees of freedom, so its mean is \(\tilde\theta=\frac{\|\sum_{i=1}^n{x_i}\|^2+d}{n}=\|\bar X_n\|^2 + \frac{d}{n}\).
        \item[(c)]
            Plugging in the values I've already derived,
            \begin{align*}
                \frac{\E_\mu\vert \theta - \tilde \theta\vert^2}{\E_\mu\vert \theta - \hat \theta\vert^2} &= \frac{\E_\mu\vert \theta - \|\bar X_n\|^2 - \frac{d}{n}\vert^2}{\E_\mu\vert \theta - \|\bar X_n\|^2 + \frac{d}{n}\vert^2} \\ 
                \frac{\E_\mu\vert \theta - \tilde \theta\vert^2}{\E_\mu\vert \theta - \hat \theta\vert^2} &= \frac{\E_\mu\vert \theta - \|\bar X_n\|^2 + \frac{d}{n} - 2\frac{d}{n}\vert^2}{\E_\mu\vert \theta - \|\bar X_n\|^2 + \frac{d}{n}\vert^2} \\
                \frac{\E_\mu\vert \theta - \tilde \theta\vert^2}{\E_\mu\vert \theta - \hat \theta\vert^2} &= \frac{\E_\mu\left((\theta - \|\bar X_n\|^2 + \frac{d}{n})^2 - 4\frac{d}{n}(\theta - \|\bar X_n\|^2 + \frac{d}{n})+4\frac{d^2}{n^2}\right)}{2d+4\theta} \\
                \frac{\E_\mu\vert \theta - \tilde \theta\vert^2}{\E_\mu\vert \theta - \hat \theta\vert^2} &= \frac{2d+4\theta+4\frac{d^2}{n^2}}{2d+4\theta} \\
                \frac{\E_\mu\vert \theta - \tilde \theta\vert^2}{\E_\mu\vert \theta - \hat \theta\vert^2} &= 1+\frac{4d^2}{n^2(2d+4\theta)} \\
            \end{align*}
            Which clearly goes to infinity, for any set $n$, as $d$ goes to infinity.
        \item[(d)]
                \begin{align*}
                    P(\mu \vert \mathcal{D}_n) &= \frac{\mathcal{L}_n(\mu) \pi(\mu)}{\int_{\R^d}\mathcal{L}_n(\mu) \pi(\mu) \, d\mu} \\
                    P(\mu \vert \mathcal{D}_n) &= \frac{\prod_{i=1}^n \frac{1}{\sqrt{(2\pi)^d}}\exp\left(-\frac{1}{2}\|x_i-\mu\|^2\right)\frac{1}{\sqrt{(2\pi\tau^2)^d}}\exp\left(-\frac{1}{2\tau^2}\|\mu\|^2\right)}{\int_{\R^d}\prod_{i=1}^n\frac{1}{\sqrt{(2\pi)^d}} \exp\left(-\frac{1}{2}\|x_i-\mu\|^2\right) \frac{1}{\sqrt{(2\pi\tau^2)^d}}\exp\left(-\frac{1}{2\tau^2}\|\mu\|^2\right)\, d\mu}  \\
                    P(\mu \vert \mathcal{D}_n) &= \frac{ \exp\left(-\frac{1}{2}\sum_{i=1}^n\|x_i-\mu\|^2-\frac{1}{2\tau^2}\|\mu\|^2\right)}{\int_{\R^d}\exp\left(-\frac{1}{2}\sum_{i=1}^n\|x_i-\mu\|^2-\frac{1}{2\tau^2}\|\mu\|^2\right) \, d\mu}  \\
                    P(\mu \vert \mathcal{D}_n) &= \frac{ \exp\left(-\frac{n}{2}\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2 -\frac{1}{2\tau^2}\|\mu\|^2 -\frac{n}{2}\left(\sum_{i=1}^n\frac{\|x_i\|^2}{n}-\left\|\sum_{i=1}^n\frac{x_i}{n}\right\|^2\right)\right)}{\int_{\R^d}\exp\left(-\frac{n}{2}\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2-\frac{1}{2\tau^2}\|\mu\|^2 -\frac{n}{2}\left(\sum_{i=1}^n\frac{\|x_i\|^2}{n}-\left\|\sum_{i=1}^n\frac{x_i}{n}\right\|^2\right)\right) \, d\mu}  \\
                    P(\mu \vert \mathcal{D}_n) &= \frac{ \exp\left(-\frac{n+\tau^2}{n2\tau^2}\|\mu-\frac{\tau^2}{n+\tau^2}\sum_{i=1}^n\frac{x_i}{n}\|^2\right)  \, d\mu}{\int_{\R^d}\exp\left(-\frac{n+\tau^2}{n2\tau^2}\|\mu-\frac{\tau^2}{n+\tau^2}\sum_{i=1}^n\frac{x_i}{n}\|^2\right)}  \\
                    P(\mu \vert \mathcal{D}_n) &= \sqrt{\frac{1}{(2\pi)^d}\left(\frac{n+\tau^2}{n\tau^2}\right)^d} \exp\left(-\frac{n+\tau^2}{n2\tau^2}\left\|\mu-\frac{\tau^2}{n+\tau^2}\sum_{i=1}^n\frac{x_i}{n}\right\|^2\right) \\
                \end{align*}
                So, we can conclude that the posterior is distributed as \(N\left(\frac{\tau^2}{n+\tau^2}\sum_{i=1}^n\frac{x_i}{n},\frac{n\tau^2}{n+\tau^2}I_d\right)\). This means that \(\frac{n+\tau^2}{n\tau^2}\mu \sim \frac{n+\tau^2}{n\tau^2}\frac{\tau^2}{n+\tau^2}\sum_{i=1}^n\frac{x_i}{n} + Z\), so $\theta$ is distributed as a non-centralized \(\chi^2\frac{n\tau^2}{n+\tau^2}\) distribution with non-centrality parameter \(\left(\frac{\tau^2}{n+\tau^2}\right)^2\left\|\bar X_n\right\|^2\). Accordingly, \(\tilde \theta = \left(\frac{\tau^2}{n+\tau^2}\right)^2\left\|\bar X_n\right\|^2+\frac{\tau^2}{n+\tau^2}\frac{d}{n}\). Plugging this into the formula from c, we get:
                \begin{align*}
                    \frac{\E_\mu\vert \theta - \tilde \theta\vert^2}{\E_\mu\vert \theta - \hat \theta\vert^2} &= \frac{\E_\mu\left\vert \theta - \left(\frac{\tau^2}{n+\tau^2}\right)^2\left\|\bar X_n\right\|^2+\frac{\tau^2}{n+\tau^2}\frac{d}{n}\right\vert^2}{\E_\mu\vert \theta - \|\bar X_n\|^2 + \frac{d}{n}\vert^2}  \\
                    \frac{\E_\mu\vert \theta - \tilde \theta\vert^2}{\E_\mu\vert \theta - \hat \theta\vert^2} &= \frac{2d+4\theta + \E_\mu\left[\left(\left(1- \left(\frac{\tau^2}{n+\tau^2}\right)^2\right)\left\|\bar X_n\right\|^2+\left(1+\frac{\tau^2}{n+\tau^2}\right)\frac{d}{n}\right)^2\right]}{2d+4\theta}  \\
                    \frac{\E_\mu\vert \theta - \tilde \theta\vert^2}{\E_\mu\vert \theta - \hat \theta\vert^2} &= \frac{O(d^2)}{2d+4\theta} 
                \end{align*}
                This quantity once again goes to infinity for a given $n$ while $d$ goes to infinity.
        \item[(e)] These are the histograms: \\
            \includegraphics[width=9cm]{hw4/4_bayes} 
            \includegraphics[width=9cm]{hw4/4_freq} 
        \item[(f)] Obviously, the frequentest estimate preforms far better than the Bayesian estimate. It appears to be the case that the mean of a posterior, even with a uniform prior, can be biased. 

        \end{itemize}
    \item[5.]
        \begin{itemize}
            \item[(a)]
                Let $\epsilon = .9^{k}$ for some $k\in\N$. Let \(Y_n=\vert\{i:i\leq n,V_i>.1\}\vert\) Since, for any $\alpha$, $P(V_j<.1)<1$,
                \begin{align*}
                    \sum_{j=1}^n w_j &= V_1 + \sum_{j=2}^n V_j \prod_{i=1}^{j-1}(1-V_i) \\
                    \sum_{j=1}^n w_j &= 1 - \prod_{j=1}^{n}(1-V_i) \\
                    P\left(\sum_{j=1}^n w_j<1-\epsilon\right) &= P\left(1 - \prod_{j=1}^{n}(1-V_i)<1-\epsilon\right) \\
                    P\left(\sum_{j=1}^n w_j<1-\epsilon\right) &= P\left(\prod_{j=1}^{n}(1-V_i)>\epsilon\right) \\
                    P\left(\sum_{j=1}^n w_j<1-\epsilon\right) &\leq P\left(\prod_{j=1}^{n}.9I_{x>.1}(V_j)>\epsilon\right) \\
                    P\left(\sum_{j=1}^n w_j<1-\epsilon\right) &\leq P\left(.9^{Y_n} >\epsilon\right) \\
                    P\left(\sum_{j=1}^n w_j<1-\epsilon\right) &\leq P\left(Y_n < k\right) \\
                    P\left(\sum_{j=1}^n w_j<1-\epsilon\right) &\leq P\left(V_j<.1\right)^{n-k} \\
                    \lim_{n\to\infty}P\left(\sum_{j=1}^n w_j<1-\epsilon\right) &\leq \lim_{n\to\infty}p\left(V_j<.1\right)^{n-k} \\
                    \lim_{n\to\infty}P\left(\sum_{j=1}^n w_j<1-\epsilon\right) &\leq 0
                \end{align*}
                Thus, the weights sum to 1 with probability 1.
            \item[(b)]
                If $X\sim F$ and $X_0\sim F_0$, let $F(x)=P(x\leq X)$ and $F_0(x)=P(x\leq X_0)$                
                    \begin{align*}
                        \E(F(x)) &= \E(\sum_{j=1}^\infty w_j I{\{z<x\}}(s_j)) \\
                        \E(F(x)) &= \sum_{j=1}^\infty \E(w_j)\E(I{\{z<x\}}(s_j)) \\
                        \E(F(x)) &= \sum_{j=1}^\infty \E(w_j)F_0(x) \\
                        \E(F(x)) &= \E(\sum_{j=1}^\infty w_jF_0(x)) \\
                        \E(F(x)) &= F_0(x) 
                    \end{align*}
                So we may conclude \(\E(F)=F_0\). To see that the prior gets more concentrated around $F_0$ as $\alpha$ increases, we only need demonstrate that the probability the drawn distribution cumulative distribution function function differs by more than $\epsilon$ from $F_0$ is decreasing as $\alpha$ increases. To do this, we note that this probability is the sum of the probabilities that the difference is achieved in exactly a certain number of steps of the stick break algorithm. This probability, conditioned on a particular sum of weights, decreases as the sum of weights decreases. Thus, since the probability of a partial sum of weights having reached a particular length decreases with $\alpha$ (as the expected size of the weights decreases), the probability of the difference occurring in a particular number of steps decreases as well, and in turn the probability of achieving that difference in any number of steps.
                \item[(c)]
                    \begin{align*}
                        P\left(\sup_x\left\vert \bar F_n(x)-F(x)\right\vert>\epsilon\right) &=P\left(\sup_x\left\vert \frac{n}{n+\alpha}F_n(x)+\frac{\alpha}{n+\alpha}F_0(x)-F(x)\right\vert>\epsilon\right) \\
                        P\left(\sup_x\left\vert \bar F_n(x)-F(x)\right\vert>\epsilon\right) &\leq P\left(\sup_x\left\vert \frac{n}{n+\alpha}F_n(x)-\frac{n}{n+\alpha}F(x)\right\vert + \sup_x\left\vert \frac{\alpha}{n+\alpha}F_0(x)-\frac{\alpha}{n+\alpha}F(x)\right\vert>\epsilon\right) \\
                        P\left(\sup_x\left\vert \bar F_n(x)-F(x)\right\vert>\epsilon\right) &\leq P\left(\frac{n}{n+\alpha}\sup_x\left\vert F_n(x)-F(x)\right\vert + \frac{\alpha}{n+\alpha}\sup_x\left\vert F_0(x)-F(x)\right\vert>\epsilon\right) \\
                        P\left(\sup_x\left\vert \bar F_n(x)-F(x)\right\vert>\epsilon\right) &\leq P\left(\frac{n}{n+\alpha}\sup_x\left\vert F_n(x)-F(x)\right\vert>\epsilon - \frac{\alpha}{n+\alpha}\sup_x\left\vert F_0(x)-F(x)\right\vert\right)
                    \end{align*}
                    Thus, using the DKW inequality,
                    \[ P\left(\sup_x\left\vert \bar F_n(x)-F(x)\right\vert>\epsilon\right)\leq 
                        \begin{cases}
                            2e^{-2n\left(\epsilon - \frac{\alpha}{n+\alpha}\sup_x\left\vert F_0(x)-F(x)\right\vert \right)^2} & \text{if } \epsilon > \frac{\alpha}{n+\alpha}\sup_x\left\vert F_0(x)-F(x)\right\vert \\
                            1 & \text{otherwise}
                        \end{cases}
                    \]
                    If the prior is really poor and $n$ isn't sufficiently large, the posterior can be arbitrarily bad.

                \item[(d)] These are my plots:
                    \begin{center}
                        \includegraphics[width=9cm]{hw4/5_d} 
                    \end{center}
                \item[(e)] 
                    \begin{itemize}
                        \item[i.] These are the plots, for $\alpha=100$ \\
                            \includegraphics[width=9cm]{hw4/5_ei10} 
                            \includegraphics[width=9cm]{hw4/5_ei25}  \\
                            \includegraphics[width=9cm]{hw4/5_ei100}
                    \end{itemize}[i.] These are the plots of the posteriors, their CIs, and the draws from them, with $\alpha=100$ \\
                            \includegraphics[width=9cm]{hw4/5_eii10} 
                            \includegraphics[width=9cm]{hw4/5_eii25}  \\
                            \includegraphics[width=9cm]{hw4/5_eii100}
        \end{itemize}
        
        
                    

        

        
\end{itemize}

\end{document}
