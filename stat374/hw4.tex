
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
STAT 374, Homework 4 \hfill Aaron Maurer
\vspace{2mm}
\hrule
\vspace{2mm}
\begin{itemize} 
    \item[1.]
        \begin{itemize} 
            \item[(a)] Averaging over $100$ simulations where $500$ points
were drawn from a normal means model with each mean equal to $2$, the average
risk of th mean was $1.016$, and the average risk of the isotonic regression
was $6.159$. As one might expect, since the MLE makes a stronger and correct
assumption, it has lower risk. However, the monotone regression still has
fairly low risk.
            \item[(b)] I estimated the risk for every fourth $n$ betwen 20 and
200. For each, I averged over 500 simulations. It seems that the risks are
proportional; the regression on the linear function seems to have about $25\%$
less risk than the step function for each $n$, with both risks growing at a
slower rate than linear, possibly proportional to \(\frac{log(n)}{n}\).
                \begin{center}
                    \includegraphics[width=9cm]{hw4/1_b} 
                \end{center}


            \item[(c)] Let $c(x_n)$ be the concave sequence of points evaluated at points $x_n$, and let $s(x_n)$ be a corresponding convex sequence. I will attempt to show that if $s(x_n)$ is not linear, it is superseded by some linear function. We can break this problem down into three cases.
                \begin{itemize}
                    \item[case 1:] If $s(x_n)>c(x_n)$ for all $x$, then there is trivially a better fit $s(x_n)-c$, where $c$ is the minimum distance between the two sequences. This improved fit is of the next case, so if there is a superseding linear fit for it, there is one for this case as well.
                    \item[case 2:] $s(x_n)=c(x_n)$ for at least one $n$, but it is never the case $s(x_n)<c(x_n)$. Let $s(x_1)=c(x_1)$. Since $s$ and $c$ are respectively convex and concave, there exists a line passing through $s(x_1)=c(x_1)$ such that $s(x_n)\geq l(x_n) \geq c(x_n) \, \forall n$. If $s(x_n)$ isn't itself a line, then there will be a $n$ for which their is strict inequality on the left hand side, making $l$ a superior fit to $s$.
                    \item[case 3:] $s(x_n)<c(x_n)$ for some $n$. Let us flesh out $s(x_n)$ and $c(x_n)$, such that $s(x)$ and $c(x)$ are piecewise linear functions connecting the points of each sequence. These functions are necessarily also convex and concave respectively. We once again have three cases:
                        \begin{itemize}
                            \item[case 3.1:] If $s(x)$ and $c(x)$ never intersect, a shift as in case 1 will achieve a superior fit and reduce the problem to the next case.
                            \item[case 3.2:] If $s(x)$ and $c(x)$ intersect exactly once, we can replace $s(x_n)$ with $s(x_n)+c$ for $n$ such that $s(x_n)<c(x_n)$, where $c$ is the minimum distance between the sequences on this set. This action will preserve convexity while creating an improved fit (the distance is strictly reduced), and reduces this case to the next case. 
                            \item[case 3.3:] If they $s(x)$ and $c(x)$ intersect at multiple points, we can choose two intersection points $a<b$. Let $l$ be the line passing through them. It is a secant line for both $s$ and $c$, so by their respective convexity/concavity, $c(x)\geq l(x) \geq s(x)$ on $[a,b]$ and $c(x)\leq l(x) \leq s(x)$ elsewhere. Thus, if $c(x)\neq l(x)$ for some $x$, then $l$ is a superior fit to the function $s$. Since $s$ and $c$ are linear between the $x_n$, this can only arise if $s(x_n)\neq l(x_n)$ for some $n$, making $l(x_n)$ a superior fit to $c(x_n)$.

                        \end{itemize}
                \end{itemize}
        \end{itemize}
    \item[2.]
        I have computed the mean squared error for each of the methods:
        \FloatBarrier
        \input{hw4_2} 
        \FloatBarrier
        \begin{itemize}
            \item[(a)]
                At a low level of noise, the wavelet methods all preform better than the local linear model. With low noise, their ability to accurately match the fine features of the Doppler function win out. None of them preform terribly though. On the other hand, once the noise increases, the James Stein and Universal Threshold methods get worse much quicker than Sure Shrink and local linear, which are comparatively much better. This is likely since it becomes harder to distinguish feature from noise. In both cases, the James Stein is worse than the Universal Threshold, and Local Linear loses out to Sure Shrink. As the graphs note the distinction between local linear regression and Sure Shrink are hard to pick out by eye  \\
                \includegraphics[width=9cm]{hw4/2_ab_pt1} 
                \includegraphics[width=9cm]{hw4/2_ab_pt01} \\
            \item[(b)]
                On the other hand, once there are occasional spikes in the variance, the local linear model preforms the best. As the graph shows, it preforms more smoothing on the occasional spikes than the Sure Shrink, which likely has them identified as true features.
                \begin{center}
                    \includegraphics[width=9cm]{hw4/2_ab_bimodal} 
                \end{center}
        \end{itemize}
    \item[3.]
        \begin{itemize}
            \item[(a)]
                I have plotted out all of the fits in the graph below. The isotonic regression provides a good fit only over through day 25 or so, when the observed values cease to be monotone. After that, the shorter fits do a reasonable job over the middle part, which is essentially flat. However, the more data the regression is fit on where the trend actually seems to be decreasing, the worse the regression preforms. Assuming the error terms are symmetrically distributed, one would expect at worst $50\%$ of the observations to fall below the actual series. One could thus generate a test statistic based on how the number of times, as the function is generated step by step, the function is forced to drop in response to a lower observed value within a particular range. This statistic would be approximately binomially distributed (though there is additional uncertainty due to the function being noisy), and a critical value could be generated from this to determine whether the range isn't monotone.
                \begin{center}
                    \includegraphics[width=9cm]{hw4/3_a} 
                \end{center}
            \item[(b)] 
                The series of local linear fits are generally pretty good. However, as they are fit in increasingly noisy parts of the data, they have a greater amount of uncertainty, since the confidence interval is fit with a constant variance assumption.
                    \includegraphics[width=9cm]{hw4/3_b25day} 
                    \includegraphics[width=9cm]{hw4/3_b50day}  \\
                    \includegraphics[width=9cm]{hw4/3_b75day} 
                    \includegraphics[width=9cm]{hw4/3_b85day}  \\
                    \includegraphics[width=9cm]{hw4/3_b171day} 
            \item[(c)]
                The Haar wavelet fit is below. Its fit is comparable on the relatively smooth portion of the series. However, in the noisy section to the end, it mostly clings to the large fluctuations in the data, while the others averaged them out in one way or another. Since these fluctuations are due to the small number of surviving flies, rather than a reflection of the true rate, this constitutes a worse fit.
                \begin{center}
                    \includegraphics[width=9cm]{hw4/3_c} 
                \end{center}
                {\textbf My Code:}
               \lstinputlisting[firstline=181,lastline=210]{hw4.R}
            \item[(d)]
                The Daubechies wavelet fit, much like the Haar, clings to closely to the observed data when the noise accounts for large swings, but otherwise has a similar, somewhat smoother, fit over the relatively calm part of the data.
                \begin{center}
                    \includegraphics[width=9cm]{hw4/3_d} 
                \end{center}
            
        \end{itemize}
    \item[4.]
        \begin{itemize}
            \item[(a)]
                \begin{align*}
                    \E(\mu \vert \mathcal{D}_n) &= \int_{\R^d} \mu  p(\mu \vert \mathcal{D}_n) \, d\mu \\
                    \E(\mu \vert \mathcal{D}_n) &= \frac{\int_{\R^d} \mu  p(\mathcal{D}_n \vert \mu) \pi(\mu) \, d\mu}{\int_{\R^d}p(\mathcal{D}_n \vert \mu) \pi(\mu) \, d\mu} \\
                    \E(\mu \vert \mathcal{D}_n) &= \frac{\int_{\R^d} \mu \prod_{i=1}^n \frac{1}{2\pi}e^{-\|x_i-\mu\|^2}  \, d\mu}{\int_{\R^d}\prod_{i=1}^n\frac{1}{2\pi} e^{-\|x_i-\mu\|^2} \, d\mu}  \\
                    \E(\mu \vert \mathcal{D}_n) &= \frac{\int_{\R^d} \mu \frac{1}{2\pi} e^{-\sum_{i=1}^n\|x_i-\mu\|^2}  \, d\mu}{\int_{\R^d}\frac{1}{2\pi}e^{-\sum_{i=1}^n\|x_i-\mu\|^2} \, d\mu}  \\
                    \E(\mu \vert \mathcal{D}_n) &= \frac{\int_{\R^d} \mu \frac{1}{2\pi} e^{-n\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2 -n\left(\sum_{i=1}^n\frac{\|x_i\|^2}{n}-\left\|\sum_{i=1}^n\frac{x_i}{n}\right\|^2\right)}  \, d\mu}{\int_{\R^d}\frac{1}{2\pi}e^{-n\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2 -n\left(\sum_{i=1}^n\frac{\|x_i\|^2}{n}-\left\|\sum_{i=1}^n\frac{x_i}{n}\right\|^2\right)} \, d\mu}  \\
                    \E(\mu \vert \mathcal{D}_n) &= \frac{\int_{\R^d} \mu \frac{1}{2\pi} e^{-n\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2}  \, d\mu}{\int_{\R^d}\frac{1}{2\pi}e^{-n\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2} \, d\mu}  \\
                    \E(\mu \vert \mathcal{D}_n) &= \int_{\R^d} \mu \frac{1}{2\pi} e^{-n\|\mu-\sum_{i=1}^n\frac{x_i}{n}\|^2}  \, d\mu\\
                    \E(\mu \vert \mathcal{D}_n) &= \sum_{i=1}^n\frac{x_i}{n}\\
                \end{align*}
        \item[(b)]
                Let \(y_i=\|x_i\|^2\) and \(\mathcal{H}_n={y_1,...,y_n}\). Since each component of each $x_i$  is independent of the others (because there is $0$ covariance and the distribution is multivariate normal), we may conclude that the $y_i$ have a $\chi^2$ distribution with $d$ degrees of freedom and a noncentrality parameter of $\|\mu\|^2$. Thus,
            \begin{align*}
                p(\theta\vert \mathcal{H}_n) &= \frac{p(\mathcal{H}_n \vert \theta)p(\theta)}{p(\mathcal{H}_n)} \\
                p(\theta\vert \mathcal{H}_n) &= \frac{\prod_{i=1}^n p(y_i \vert \theta)\int_{\R^d} p(\theta \vert \mu) p(\mu) d\mu}{\int_{\R^d}p(\mathcal{H}_n\vert \mu)p(\mu)} \\
                p(\theta\vert \mathcal{H}_n) &= \frac{\prod_{i=1}^n p(y_i \vert \theta)\int_{\R^d} I_{\{\|x\|^2=\theta\}}(\mu) d\mu}{\int_{\R^d}\prod_{i=1}^n p(y_i \vert \theta) d\mu)} \\
            \end{align*}
        \end{itemize}
    \item[5.]
        \begin{itemize}
            \item[(a)]
                \begin{align*}
                    \sum_{j=1}^n w_j &= V_1 + \sum_{j=2}^n V_j \prod_{i=1}^{j-1}(1-V_i) \\
                    \sum_{j=1}^n w_j &= 1 - \prod_{j=1}^{n}(1-V_i) \\
                    \sum_{j=\infty}^n w_j &= 1 - \prod_{j=1}^{\infty}(1-V_j) \\
                \end{align*}
                
        \end{itemize}

        

        
\end{itemize}

\end{document}
