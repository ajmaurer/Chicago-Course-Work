
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%% AMS PACKAGES - Chances are you will want some or all of these if writing a math dissertation.
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, enumerate, multicol, graphicx, listings}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
STAT 374 HW3 \hfill Aaron Maurer
\vspace{2mm}
\hrule
\vspace{2mm}
\begin{itemize}
    \item[1.]
        \begin{itemize}
            \item[a)]
                \begin{align*}
                    \E[\hat p_{n,s}(x_0)] &= \E\left[\frac{1}{nh^{s+1}}\sum_{i=1}^n K\left(\frac{X_i-x_0}{h}\right)\right] \\
                                        &= \frac{1}{h^{s+1}}\int K\left(\frac{u-x_0}{h}\right)p(u) du\\
                                        &= \frac{1}{h^{s}}\int K\left(u\right)p(x_0+hu) du\\
                                        &= \frac{1}{h^{s}}\int K(u)\left(\sum_{j=0}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0)\right) du\\
                                        &= \sum_{j=0}^{l-1}\frac{h^{j-s}}{j!}p^{(j)}(x_0)\int K(u)u^j du + \frac{1}{h^{s}}\int K(u)\left(\sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0)\right) du\\
                                        &= p^{(s)}(x_0)+ \frac{1}{h^{s}}\int K(u)\left(\sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0)\right) du
                \end{align*}
                Using the Lagrange form of the remainder of a Taylor series, for some $\xi\in(x_0,x_0+hu)$,
                \[\sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0) = \frac{(hu)^l}{l!}p^{(l)}(\xi)\]
                By the conditions of the H{\"o}lder class,
                \begin{align*}
                    \sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0) &\leq \frac{(hu)^l}{l!}(L\vert \xi - x_0\vert^{\beta-l} + \vert p(x_0)\vert) \\
                    \sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0) &\leq \frac{(hu)^l}{l!}(L\vert hu\vert^{\beta-l}  + \vert p(x_0)\vert) \\
                    \sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0) &\leq h^\beta\frac{u^l\vert u \vert^{\beta-l}L}{l!}+ u^l\frac{h^l}{l!}\vert p(x_0)\vert \\
                    \sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0) &\leq h^\beta f_1(u)+ u^lf_2(x_0) \\
                \end{align*}
                For \(f_1(u):=\frac{u^l\vert u \vert^{\beta-l}L}{l!}\) and \(f_2(x_0):=\frac{h^l}{l!}\vert p(x_0)\vert\). By similar logic, we also get
                \begin{align*}
                    \sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0) &\geq -\frac{(hu)^l}{l!}(L\vert \xi - x_0\vert^{\beta-l} + \vert p(x_0)\vert) \\
                    \sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0) &\geq -h^\beta f_1(u)- u^lf_2(x_0) 
                \end{align*}

                Plugging these into our expression for \(\E[\hat p_{n,s}(x_0)]\), we get that 
                \begin{align*}
                    \E[\hat p_{n,s}(x_0)] &= p^{(s)}(x_0)+ \frac{1}{h^{s}}\int K(u)\left(\sum_{j=l}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0)\right) du \\
                    \vert \E[\hat p_{n,s}(x_0)]-p^{(s)}(x_0) \vert &\leq \frac{1}{h^{s}}\int K(u) (h^\beta f_1(u)+ u^lf_2(x_0)) du \\
                                                                &\leq h^{\beta-s}\int K(u) f_1(u) du + \frac{f_2(x_0)}{h^{s}}\int K(u) u^l du\\
                                                                &\leq h^{\beta-s}c
                \end{align*}

                For \(c:=\int K(u) f_1(u) du\), showing the bias is bounded as desired. To bound the variance term, we have that
                \begin{align*}
                    \var\left(\hat p_{n,s}(x_0)\right) &= \frac{1}{nh^{2s+2}}\var\left(K\left(\frac{X_i-x_0}{h}\right)\right) \\
                                                     &= \frac{1}{nh^{2s+2}}\left(\E\left[K\left(\frac{X_i-x_0}{h}\right)^2\right]-\E\left[K\left(\frac{X_i-x_0}{h}\right)\right]^2\right) \\
                                                     &\leq \frac{1}{nh^{2s+2}}\left(h\int K^2(u)\left(\sum_{j=0}^\infty \frac{(hu)^j}{j!}p^{(j)}(x_0)\right)du-\left(h^{s+1}p^{(s)}(x_0)+ h^{\beta+1}c\right)^2\right) \\
                                                     &\leq \frac{p(x_0)}{nh^{2s+1}}\int K^2(u)du + O\left(\frac{1}{n}\right)
                \end{align*}
                So \(\var\left(\hat p_{n,s}(x_0)\right)\) is bounded by \(\frac{c'}{nh^{2s+1}}\) for \(c'>0\).
            \item[b)]
                Combining the bounds on bias squared and variance from part a) to get the risk, we get
                \begin{align*}
                    \mbox{max}(\L^2(\hat P_{n,s}(x))) &= c^2h^{2(\beta-s)}+\frac{c'}{nh^{2s+1}} \\
                    \frac{d}{dh}\mbox{max}((\L^2(\hat P_{n,s}(x))) &= 2(\beta-s)c^2h^{2(\beta-s)-1}-\frac{(2s+1)c'}{nh^{2s+2}} 
                \end{align*}
                Then, this risk bound is minimized when its derivative is at $0$, so for the optimal $h$,
                \begin{align*}
                    0 &= 2(\beta-s)c^2h^{2(\beta-s)-1}-\frac{(2s+1)c'}{nh^{2s+2}} \\
                    \frac{(2s+1)c'}{2(\beta-s)c^2}\frac{1}{n} &= h^{2\beta+1} \\
                    \left(\frac{(2s+1)c'}{2(\beta-s)c^2}\right)^\frac{1}{2\beta+1}n^{-\frac{1}{2\beta+1}} &= h 
                \end{align*}
                Plugging this into the original risk function, and defining a new constant \(c_1=\left(\frac{(2s+1)c'}{2(\beta-s)c^2}\right)^\frac{1}{2\beta+1}\), we get
                \begin{align*}
                    \mbox{max}(\L^2(\hat P_{n,s}(x)))  &= c^2c_1^{2(\beta-s)}n^{\frac{2(\beta-s)}{2\beta+1}}-\frac{c'}{c^{2s+1}}n^{\frac{2s+1-2\beta+1}{2\beta+1}} \\
                    \mbox{max}(\L^2(\hat P_{n,s}(x))) &= \left(c^2c_1^{2(\beta-s)}+\frac{c'}{c^{2s+1}}\right)n^{\frac{2(\beta-s)}{2\beta+1}}
                \end{align*}
                So the maximum risk is of order \(O\left(n^{\frac{2(\beta-s)}{2\beta+1}}\right)\).
        \end{itemize}
    \item[2.]
        \begin{itemize}
            \item[a)]
                \begin{align*}
                    L_F(x)&=\lim_{\epsilon\to 0} \frac{T((1-\epsilon)F+\epsilon\delta_x)-T(F)}{\epsilon} \\
                    L_F(x)&=\lim_{\epsilon\to 0} \frac{((1-\epsilon)F(b)+\epsilon\delta_x(b))-((1-\epsilon)F(a)+\epsilon\delta_x(a))-(F(b)-F(a))}{\epsilon} \\
                    L_F(x)&=\lim_{\epsilon\to 0} \frac{\epsilon(I_{[x,\infty)}(b)-I_{[x,\infty)}(a))-\epsilon (F(b)-F(a))}{\epsilon} \\
                    L_F(x)&= I_{[a,b)}(x)-T(F) \\
                \end{align*}
            \item[b)]
                \begin{align*}
                    \hat{se}(\hat \theta) &= \frac{\hat \tau}{\sqrt{n}} \\
                    \hat{se}(\hat \theta) &= \frac{1}{n^\frac{3}{2}}\sum_{i=1}^n(I_{[a,b)}(X_i)-T(\hat F_n))^2 \\
                    \hat{se}(\hat \theta) &= \frac{1}{n^\frac{3}{2}}\sum_{i=1}^n(I_{[a,b)}(X_i)-\hat F_n(b)+\hat F_n(a))^2 \\
                    \hat{se}(\hat \theta) &= \frac{1}{\sqrt{n}}\left((\hat F_n(b)-\hat F_n(a))(1-\hat F_n(b)+\hat F_n(a))^2+(1-\hat F_n(b)+\hat F_n(a))(\hat F_n(b)-\hat F_n(a))^2\right) \\
                    \hat{se}(\hat \theta) &= \frac{1}{\sqrt{n}}\left((\hat F_n(b)-\hat F_n(a)+1- \hat F_n(b)+\hat F_n(a))(\hat F_n(b)-\hat F_n(a))(1-\hat F_n(b)+\hat F_n(a))\right) \\
                    \hat{se}(\hat \theta) &= \frac{1}{\sqrt{n}}\left((\hat F_n(b)-\hat F_n(a))(1-\hat F_n(b)+\hat F_n(a))\right) \\
                \end{align*}
            \item[c)]
                An approximate \(1-\alpha\) confidence interval is given by 
                \[\hat \theta \pm z_{\frac{\alpha}{2}}\hat{se}(\hat \theta) = F_n(b)-\hat F_n(a) \pm z_{\frac{\alpha}{2}}\frac{1}{\sqrt{n}}\left((\hat F_n(b)-\hat F_n(a))(1-\hat F_n(b)+\hat F_n(a))\right)\]
            \item[d)]
                To estimate \(se(\hat \theta)\), one would draw a sample, with replacement, of $n$ from the $X_i$s. For each of these samples, $\hat\theta$ would be calculated (this is the portion of the sample in \([a,b)\). Then, \(se(\hat \theta)\) is the standard deviation of $\hat\theta$ among all the samples.

        \end{itemize}
    \item[3.]
        \begin{itemize}
            \item[a)]
                I fit a kernel density estimate with a Gaussian kernel and bandwidth chosen by the Normal Reference Rule. I didn't use cross validation, since the rounding in the magnitudes made it so that cross validation would default to an arbitrarily small bandwidth (as we discovered in our last homework).
                \begin{center}
                    \includegraphics[width=9cm]{hw3_3_a}
                \end{center}
            \item[b)]
                Below I have plotted the estimated CDF along with a $95\%$ confidence interval. This band is generated from \(\hat F(x) \pm \sqrt{\frac{1}{2n}\log\left(\frac{2}{.05}\right)}\), winsorized above at $1$ and below at $0$.
                \begin{center}
                    \includegraphics[width=9cm]{hw3_3_b}
                \end{center}
            \item[b)]
                Using the density from problem a, we get estimate for \(F(4.6)-F(4.3)\) of 
                \[\int_{4.3}^{4.6} f(x)dx = .533 \]
                Using the plug-in estimator, we get \(\hat F(4.6) - \hat F(4.3)=.526\). Then, using the result from problem 2, we get an approximate confidence interval of \(.526\pm z_{.025}\frac{.526(1-.526)}{\sqrt{1000}} = (.522,.530)\). 

        \end{itemize}
    \item[4.]
        \begin{itemize}
            \item[a)]
                To get the analytical distribution of \(\hat \theta\), its easiest to start by calculating the distribution function. For any $x\in[0,\theta]$
                \begin{align*}
                    P(\hat \theta \leq x) &= P(X_1\leq x \cap ... \cap X_n\leq x) \\
                    P(\hat \theta \leq x) &= \prod_{i=1}^n P(X_i\leq x) \\
                    P(\hat \theta \leq x) &= \prod_{i=1}^n \left(\frac{x}{\theta}\right)^n
                \end{align*}
                Differentiating both sides, we get the distribution function \(f(x)=\frac{nx^{n-1}}{\theta^n}\). Below, I drew a sample of $50$ from a uniform with $\theta=1$, and then preformed a parametric and nonparametric bootstrap of $\hat\theta$: \\
                \includegraphics[width=9cm]{hw3_4_a_np}
                \includegraphics[width=9cm]{hw3_4_a_p} \\
                As you can see, the parametric bootstrap gives a distribution fairly close to the analytic one (just shifted a little to the left), while the nonparametric bootstrap fails to do so, having the majority of the point mass at the max from the original sample.
            \item[b]
                Since in the parametric case \(X_i^*\sim \mbox{Uniform}(0,\hat\theta)\),
                \begin{align*}
                    P(\hat\theta^*=\hat\theta) &= P(\exists i, X_i^*=\hat\theta\cap \forall j, X_j^*\leq\hat\theta) \\
                    &\leq P(\exists i, X_i^*=\hat\theta) \\
                    &\leq \prod_{i=1}^n (1-P(X_i^*=\hat\theta)) \\
                    &\leq 0
                \end{align*}
                On the other hand, in the nonparametric case,
                \begin{align*}
                    P(\hat\theta^*=\hat\theta) &= P(\exists i, X_i^*=\hat\theta\cap \forall j, X_j^*\leq\hat\theta) \\
                    &= P(\exists i, X_i^*=\hat\theta) \\
                    &= 1-P(\forall i, X_i^*\neq\hat\theta) \\
                    &= 1-\left(1-\frac{1}{n}\right)^{n} \\
                \end{align*}
                For $n=50$, this gives us $.636$, but in the limiting case, we get 
                \begin{align*}
                    \lim_{n\to\infty}P(\hat\theta^*=\hat\theta)&= 1-\lim_{n\to\infty}\left(1-\frac{1}{n}\right)^{n}  \\
                                                               &= 1-\lim_{n\to\infty}\left(\frac{n}{n-1}\right)^{-n}  \\
                                                               &= 1-\lim_{n\to\infty}\left(1+\frac{1}{n-1}\right)^{-n}  \\
                                                               &= 1-\lim_{n\to\infty}\left(1+\frac{1}{n-1}\right)\left(1+\frac{1}{n-1}\right)^{-n+1}  \\
                                                               &= 1-\frac{1}{e}  \\
                                                               &\approx .632  \\
                \end{align*}
        \end{itemize}
    \item[5.]
        \begin{itemize}
            \item[a)]
                For this model, the bootstrapped $95\%$ confidence interval for \(\theta\) I got was \((.240,.282)\), and the "true" $\theta$, which was the result of running the experiment for $n=500$, was $.264$. Thus, the confidence interval included the "true" parameter, and also the real \(\theta=.25\).
            \item[b)]
                For this model, my confidence interval was \(-.280,.120)\) and the "true" $\theta$ was \(.023\). Thus, the confidence interval covered the "true" parameter and also the real parameter \(\theta=0\).
            \item[c)]
                For this model, my confidence interval was \((.282,.482)\) with the "true" $\theta=.865\). Thus, my confidence interval was far below the "true" $\theta$, and even farther below the actual $\theta$ of $0$. 
        \end{itemize}


\end{itemize}
\end{document}
