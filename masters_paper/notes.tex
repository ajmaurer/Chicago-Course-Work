
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\linespread{1.5}
\pdfpagewidth 8.5in
\pdfpageheight 11in

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\p}{\mathrm{P}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\x}{\bf x}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newcommand{\hwhead}[1]{#1 \hfill Aaron Maurer \vspace{2mm} \hrule \vspace{2mm}}

\begin{document}
\title{Binary Knockoffs Notes}
\author{Aaron Maurer}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries} 
{\em Note:} I will try to hold to the convention that $X$ is the $n\times p$ data matrix, while $\bf x$ is the random vector variable from which each row of $X$ was drawn. Accordingly, $\tilde X$ will be the knockoff matrix while $\bf \tilde x$ is a random vector variable. $x_i$ will be the random variable corresponding to the $i$th entry of $\bf x$, while $X_i$ is the vector of observations drawn from $x_i$ in the data matrix. \\

Some early investigation into deterministic knockoffs (as described in the original knockoff paper) reveal that they don't perform well in L1 regularized logistic regression. Even when $X_i$ is a null predictor of $y$, the $X_i$ still tend to enter the model prior to $\tilde X_i$. The issue is that even when $x\sim N_p({\bf 0}, \Sigma)$ for some $\Sigma\succeq 0$, $\tilde x$ is not normally distributed. This can be seen from producing qq plots of $X_i$ vs $\tilde X_i$ for each $i$. Of course, when $X$ is a binary vector, $\tilde X$ completely doesn't match its distribution, causing the original $X$ to beat the knockoffs into the model. This indicates that a new method of generating $\tilde X$ must be created to control FDR via knockoffs with regularized logistic regression. 

\section{Probabilistic Random Bernoulli Knockoffs}
My idea is to generate $\bf \tilde x$ randomly such that, approximately, $\bf \tilde x\sim \bf x$. In particular, both variables should have similar marginal densities, expectations, and second moments. However, $\bf \tilde x\st \bf x$ should also have desired knockoff property that $\E(\bf \tilde x'\bf x\st \bf x) = \bf x'\bf x-s$, where $\diag(\bf x'\bf x)-s$ is small. In the general case, this is likely infeasible; however, if $\bf x$ is a binary vector, as is often the case, we know we are dealing with a much more limited class of random variables, and it should be possible to randomly generate $\bf \tilde x\st \bf x$ so as to have the desired properties. At worst, this method will provide a suitable replacement for deterministic $\bf \tilde x$ for use with LASSO, and if we are lucky, it will work reasonably for other regularized GLMs. 

\section{Random Bernoulli Generation}
Thankfully, there has been a reasonable amount of work on how one can generate random Bernoulli vectors with some kind of correlation among among the values. A random Bernoulli vector $\bf x$ can be summarized by its first two moments: a mean vector $\E(\bf x)=m\in(0,1)^p$ and cross-moment matrix $\E(\bf x\bf x')=M\in(0,1)^{p\times p}$. Obviously, $m_i = \p(x_i=1)$, $M_{ij} = \p(x_i=x_j=1)$, and $m=\diag(M)$. For an arbitrary symmetric $M$ to be valid cross-moment matrix, $M-mm'\succeq0$, and
\[ \max\{0,m_i+m_j -1\} \leq M_{ij} \leq \min\{m_i,m_j\}\]
for all $i\neq j$\footnote{``On parametric families for sampling binary data with specified mean and correlation" - http://arxiv.org/abs/1111.0576}. Given a qualifying $M$, or observed $X$, there are a few ways of generating more random $\bf x$.

\subsection{Gaussian Copula Family}
Since multivariate normal distributions are easy to randomly draw, the idea is to find some random normal variable $z\sim N_p({\bf 0},\Sigma)$ such that,for $x_i = I(z_i<0)$, $x$ has the desired properties. There are a number of ways to do this\footnote{``On the Generation of Correlated Artificial Binary Data" - http://epub.wu.ac.at/286/1/document.pdf}\footnote{``On parametric families for sampling binary data with specified mean and correlation"}, but it turns out that there is only certain to exist a working $\Sigma$ in the bivariate case. 

\subsection{$\mu$-Conditionals family}
It turns out that there exists a more flexible family which will always work for arbitrary $M$ called $\mu$-conditionals. The basic idea is that the $X$ is generate sequentially as 

\[ \p(x_{i}=1\st x_{1},...,x_{i-1}) = \mu\left(a_{ii}+\sum_{k=1}^{i-1}a_{ik}x_i\right) \]

for some monotone function $\mu:\R\to[0,1]$. This is essentially a binomial family GLM for a link function $\mu$. If one takes all of the $a_{kj}$, they can form a lower triangular matrix $A$, and then the joint density can be expressed as 

\[ \p(\mathbf x=\mathbf \gamma) \propto \mu(\mathbf{\gamma}' A\mathbf \gamma)\]

If $\mu$ is chose such that it is a bijection and differentiable, there is a unique $M$ such that $\E(x_ix_i')=M$\footnote{``On parametric families for sampling binary data with specified mean and correlation"}. It turns out that the natural choice for $\mu$ is the logistic link function, which yields the Ising model, the ``binary analogue of the multivariate normal distribution which is the maximum entropy distribution on $\R^p$ having a given covariance matrix." Additionally, it has the usual benefit that the coefficients can be viewed as a log odds ratio:
\[a_{ij} = \log\left(\frac{\p(x_j=x_k=1)\p(x_j=x_k=0)}{\p(x_j=0,x_k=1)\p(x_j=1,x_k=0)}\right) \]
when $i\neq j$. I think this dictates that if $\bf x$ is generated from this model with $a_{ij}=0$, then $x_i$ and $x_j$ are independent.  \par

There is no closed form to calculate the entries in $A$ if $p>1$, but they can be derived numerically two ways.  
\begin{enumerate}
    \item If one is attempting to replicate the empirical cross-moments from a data matrix $X$, $a_{1i}$ to $a_{ii}$ can be derived from fitting successive logistic regressions of $X_i$ on $X_{1} \ldots X_{i-1}$ using maximum likelihood. $a_{ji}$ for $i\neq j$ will then be the coefficient on $X_j$ while $a_{ii}$ is the intercept of the regression.
    \item If one is just working with a desired cross-moment matrix $M$, the successive rows of $A$ can be fit via Newton-Raphson. \par
        Let us say that the first $i-1$ rows have already been fit, resulting in the upper left $(i-1)\times(i-1)$ sub matrix $A_{-i}$ of $A$. Let us say that $\mathbf a_i$ is the first $i$ entries of the $i$th row of $A$ (the rest will be 0 anyway). As well, let $\mathbf m_i$ be similarly the first $i$ entries of the $i$th row of $M$. In other words, $\mathbf{m}_i = [\E(x_i x_j)]_{j=1}^{i}$. Finally, let us say that $\mathbf x_{-i}$ is the first $i-1$ entries of $\mathbf x$. We want to solve for $\mathbf a_i$ such that
        \begin{align*}
            \mathbf m_i &= \E\left(x_i \left[\begin{array}{c} \mathbf x_{-i} \\ x_i \end{array}\right]\right) \\
            \mathbf m_i &= \E\left(\E\left(x_i \left[\begin{array}{c} \mathbf x_{-i} \\ x_i \end{array}\right]\;\bigg \vert\; \mathbf x_{-i}\right)\right)\\
            \mathbf m_i &= \sum_{\mathbf x_{-i} \in \{0,1\}^{i-1}} \p(\mathbf x_{-i})\p(x_i=1\st \mathbf x_{-i}) \left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right] \\
    \mathbf m_i &= \sum_{\mathbf x_{-i} \in \{0,1\}^{i-1}} \frac{1}{c}\mu\left(\mathbf x_{-i}'A_{-i}\mathbf x_{-i}\right) \mu\left(\mathbf{a}_i'\left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right] \\
        \end{align*}
        Where $c$ is the appropriate normalizing constant. Let us define the quantity on the right in the last line as $f(\mathbf a_i)$. We can solve for $f(\mathbf a_i)=\mathbf{m}_i$ by successive Newton-Raphson iterations defined by 
        \[\mathbf a_i^{(k+1)} = \left[H f\left(\mathbf a_i^{(k)}\right)\right]^{-1}\left[f\left(\mathbf a_i^{(k)}\right)-\mathbf m_i\right] \]
        The Hessian matrix is calculated as 
    \[ H f\left(\mathbf a_i\right) = \sum_{\mathbf x_{-i} \in \{0,1\}^{i-1}} \frac{1}{c}\mu \left(\mathbf x_{-i}'A_{-i}\mathbf x_{-i}\right) \mu'\left(\mathbf{a}_i'\left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right]\left[\begin{array}{cc} \mathbf x_{-i}' & 1 \end{array}\right] \]
        With $2^{i-1}$ possible values for $\mathbf x_{-i}$, this can quickly become computationally expensive. Instead, with a series of values $\mathbf x_{-i}^{(k)}\sim \mathbf x_{-i}$, we can approximate
        \[f\left(\mathbf a_i\right) \approx \frac{1}{N}\sum_{k=1}^N \mu\left(\mathbf{a}_i'\left[\begin{array}{c} \mathbf x_{-i}^{(k)} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_{-i}^{(k)} \\ 1 \end{array}\right] \]
        and
        \[H f\left(\mathbf a_i\right) \approx \frac{1}{N}\sum_{k=1}^N \mu'\left(\mathbf{a}_i'\left[\begin{array}{c} \mathbf x_{-i}^{(k)} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_{-i}^{(k)} \\ 1 \end{array}\right]\left[\begin{array}{cc} \mathbf [x_{-i}^{(k)}]' & 1 \end{array}\right] \]

\end{enumerate}

\section{Generating Knockoffs}

\end{document}
