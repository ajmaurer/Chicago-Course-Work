
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\linespread{1.5}
\pdfpagewidth 8.5in
\pdfpageheight 11in

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\p}{\mathrm{P}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\diag}[1]{\mathrm{diag}\{#1\}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\x}{\bf x}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newcommand{\hwhead}[1]{#1 \hfill Aaron Maurer \vspace{2mm} \hrule \vspace{2mm}}

\begin{document}
\title{Using Probabilistic Knockoffs of Binary Variables to Control the False Discovery Rate}
\author{Aaron Maurer}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
    Variable selection is an essential problem to fitting a regression model. For linear regression, the knockoff filter (Barber and Cand\`{e}s, 2014) offers a method of exact FDR control. However, the method as originally formulated does not extend to other generalized linear models such as logistic regression. This paper offers an extension of this method which will work with binary features in the context of linear regression, but also will extend to other GLMs.  
    
\subsection{Knockoff Filter}
    Let us consider the consider the usual setting for linear regression, where $n$ observations of a variable of interest $y$ arise from the model
    \[\mathbf{y} = X\beta + \mathbf{z}\]
    where $\mathbf{y}\in\R^n$ are the observed values of $y$, $X \in \R^{n\times p}$ is the matrix of predictor variables, $\beta\in\R^p$ are the unknown coefficients, and $\mathbf z$ is Gaussian noise. For the purpose of this paper, only the case where $2p\leq n$ will be considered, though the methods can be extended to $p\leq n$. I will to refer the $j$th column of $X$ as $X_j$, which is $n$ observations of the random variable $x_j$. The random variable $x_j$ is in turn $j$th entry of the random vector valued variable $\mathbf x$. In the situation where $\beta$ is sparse and there is reason to expect that $x_j$ and $y$ are uncorrelated for many $j$, the knockoff filter is a method to select a subset of the $x_j$ which are likely correlated with $y$. \par
    The method specifically seeks to control the false discovery rate (FDR), which is the expected portion of some selection of $x_j$ which are, in fact, uncorrelated with $y$. For a procedure which chooses a subset of variables $\hat S\subseteq \{1,\ldots,p\}$, this is defined as  
    \[ \textrm{FDR} = \E\left[\frac{\vert{\{j: \beta_j=0 \; \& \; j\in\hat S\}}\vert}{\max\{\vert{\hat S}\vert,1\}} \right] \]
    FDR is controlled at level $q$ if FDR is less than $q$ irrespective of the coefficients $\beta$. \par

    The knockoff filter achieves FDR control in several steps, the first of which is creating a set of `knockoff' features $\tilde X_j$ which imitate the original $X_j$ while being less correlated with $y$. In particular, the matrix of knockoffs $\tilde X$ has the same internal correlation structure as $X$, the knockoff feature $\tilde X_j$ has the same correlation with other $X_i$ as $X_j$ does, but the correlation between $\tilde X_j$ and $X_j$ is minimized. In other words, where $\hat \Sigma := X^TX$, 
    \[ \tilde X^T\tilde X=\hat\Sigma \textrm{ and } \tilde X^T X = \Sigma - \diag{\mathbf s}\]
    for some vector $s\in \R^p$ such that, where $\diag{\Sigma}$ is the vector made from the main diagonal of $\Sigma$, $\diag{\Sigma} - \mathbf s$ is small. Since $\tilde X_j$ and $X_j$ have relatively low correlation, as long as $X_j$ is created independently of $\mathbf y$, $\tilde X_j$ will have lower correlation with $\mathbf y$ than $X_j$. In the case where $X_j$ is uncorrelated with $\mathbf y$, $\tilde X_j$ will also be uncorrelated with $\mathbf y$. How these knockoffs are actually generated will be described shortly.\par
    The second step is to fit a series of Lasso model of $\mathbf y$ on the combined design matrix $[X \; \tilde X]$ so as to determine the largest value $\lambda$ at which the coefficient for each $X_j$ and $\tilde X_j$ is nonzero.\footnote{Other statistics besides the $\lambda$s can also be used in the knockoff method, but are not considered in this paper} Recall, for a given $\lambda$, the estimated coefficient from the Lasso regression will be 
    \[ \beta(\lambda) = \textrm{argmin}_\mathbf b \left\{\frac{1}{2}\|\mathbf y - [X\; \tilde X]\mathbf b\|^2_2 + \lambda\|b\|_1 \right\}\]
    In the general case, the coefficient for one feature in a Lasso regression being first nonzero for a larger value of $\lambda$ than another feature is an indication that the first feature is a stronger predictor of the outcome than the second. Thus, since, by construction, the knockoff features are weaker predictors than the originals, we would expect the original features to have nonzero coefficients sooner than the knockoff features when the original is a valid predictor. On the other hand, since $\beta(\lambda)$ only depends on $[X\; \tilde X]$ through the sufficient statistics $[X\; \tilde X]^T[X\; \tilde X]$ and $[X\; \tilde X]^T\mathbf y$, for null predictors, the coefficients of the original features won't enter any sooner on average than knockoff feature. This can be seen by switching a null $X_j$ with $\tilde X_j$; by construction, $[X\; \tilde X]^T[X\; \tilde X]$ will be unaltered, while $\E\left[[X\; \tilde X]^T\mathbf y\right]$ will also be unchanged, since $\E[X_j^T\mathbf y]=\E[\tilde X_j^T\mathbf y]=0$. \par
   This observation leads to the final step. Let $Z_j$ be the largest $\lambda$ such that feature $j$ has a nonzero coefficient and $\tilde Z_j$ the largest $\lambda$ such that knockoff $j$ has a nonzero coefficient. Then, define $W_j$ as 
   \[ W_j = \begin{cases}   Z_j         &\mbox{if } Z_j>\tilde Z_j \\ 
                            -\tilde Z_j &\mbox{if } Z_j<\tilde Z_j \\
                            0           &\mbox{if } Z_j=\tilde Z_j \\
            \end{cases} \]
            When $X_j$ is a null predictor, $W_j$ will be symmetrically distributed around zero, and when $X_j$ is true predictor, it will have a distribution skewed positive. Since $W_j$ with large positive values are more likely to be true predictors, the knockoff filter selects the variables $\{j:W_j\geq T\}$, where $T$ is a threshold defined as
            \[ T = \min\left\{ t>0 \;: \; \frac{\vert\{j:W_j\leq -t\}\vert}{\max\{\vert\{j:W_j\geq t\}\vert,1\}}\leq q \right\} \]
            Basic logic of this method is fairly simple. Let $N_h(t)$ be the random variable for the number of null predictors with $W_j\leq t$, $N_l(t)$ be the number of null predictors with $W_j\geq t$, and $V_l(t)$ \& $V_h(t)$ be the similar number of true predictors. Since the null $W_j$ are symmetrically distributed around $0$, $\E[N_l(t)]=\E[N_h(t)]$. Thus,
            \begin{align*}
                q &\geq \E\left[ \frac{\vert\{j:W_j\leq -t\}\vert}{\max\{\vert\{j:W_j\geq t\}\vert,1\}} \right] \\
                q &\geq \E\left[ \frac{N_l(t) + V_l(t)}{\max\{N_h(t) + V_h(t),1\}} \right] \\
                q &\geq \E\left[ \frac{N_l(t)}{\max\{N_h(t) + V_h(t),1\}} \right] \\
                q &\geq \E\left[ \frac{N_h(t)}{\max\{N_h(t) + V_h(t),1\}} \right] \\
                q &\geq \textrm{FDR}
            \end{align*}
            So FDR is controlled, and since $V_l(t)$ will tend to be small, it should be controlled fairly tightly.

\subsection{Original Knockoff Features}
    The original formulation of the method offers two similar methods of constructing knockoffs. Both of these will, by construction, have exactly the property that
    \[ \tilde X^T\tilde X=\hat\Sigma \textrm{ and } \tilde X^T X = \Sigma - \diag{\mathbf s}.\]
    For both methods, the first step is to normalize the matrix $X$ such that $X_j^TX_j=1$ for all $j$. Then, let the Gram matrix of $[X\; \tilde X]$ be
    \[ G:= [X\; \tilde X]^T[X\; \tilde X] = \left[ \begin{array}{cc} \Sigma & \Sigma - \diag{\mathbf s} \\ \Sigma - \diag{\mathbf s} & \Sigma \end{array}\right] \]
    $A$, the Schur complement of $\Sigma$ in $G$ can be calculated as 
    \[ A = 2\,\diag{\mathbf s} - \diag{\mathbf s}\Sigma^{-1}\diag{\mathbf s} \]
    For $G$ to exist, $G$ must be positive semi-definite, which happens if and only if $A$ is positive semi-definite, which happens in turn if and only if\footnote{A fleshed out version of this argument can be found in the original knockoff paper} 
    \[ \diag{\mathbf s} \succeq 0  \; \textrm{ and } \; 2\Sigma - \diag{\mathbf s} \succeq 0 \]
    Given this is true, $A$ can be factored as $A=C^TC$. Combining this with a satisfactory $\mathbf s$ and an orthonormal matrix $\tilde U\in\R^{n\times p}$ such that $\tilde U^T X = 0$, a $\tilde X$ fulfilling the desired properties can be calculated as 
    \[ \tilde X = X(I-\Sigma^{-1}\,\diag{\mathbf s}) + \tilde UC\]
    As mentioned above, $\mathbf s$ should be chosen so as to make $\diag{\Sigma}-\mathbf s$ small. Since each $X_j$ has been normalized, this means that $\diag{\Sigma}=\mathbf 1$, so $\mathbf 1 - \mathbf s$ should be minimized in accordance with the restrictions on $\mathbf s$. The two methods differ in how $\mathbf s$ is chosen:
    \begin{itemize}
        \item \textit{Equi-correlated knockoffs}: Each original features is set to have the same correlation with its knockoff by setting $\mathbf s = 2\min\{\lambda_{min}(\Sigma),1\}\mathbf 1$. For all simulations in this paper, this was the method used.
        \item \textit{SDP knockoffs}: The $\mathbf s$ which minimizes the average correlation between knockoff and original features can be found via a semi-definite programing problem:
            \begin{center}
                \begin{tabular}{r l}
                    minimize & $\|\mathbf1-\mathbf s\|_1$ \\
                    subject to & $ 0 \preceq \diag{\mathbf s} \preceq 2\Sigma $
                \end{tabular} 
            \end{center}
            This method is significantly more computationally intensive. 
    \end{itemize}
     
\subsection{Binary Knockoffs}
    Though the ``original" knockoff method, described above, achieves the exact desired knockoff properties, the individual values in the vector $X_j$ will have little relation to the individual values in $X_j$. In particular, these values will often have very different empirical distributions. This effect is particularly noticeable when the random variable $x_j$ is discrete but $\tilde X_j$ does not even consist of the same set of values. The end result is that for variable selection for other generalized linear models, which do not have the same sufficient statistics as linear regression which knockoffs are designed to hit, the original knockoffs will fail to control FDR. \par
    Thus, this paper offers a new method of generating knockoffs which should offer superior performance with other GLMs. In this new method, the matrix of knockoff features $\tilde X$ will be generated by row from a random vector valued variable $\mathbf{\tilde x}$, each entry $\tilde x_j$ of which will have the property $\tilde x_j \sim x_j$. This random variable will also abide by a relaxation of the original knockoff condition:
    \[ \E[\mathbf{\tilde x}^T\mathbf{\tilde x}]=\hat\Sigma \; \textrm{ and } \; \E[\mathbf{\tilde x}^T \mathbf x \st \mathbf x] = \Sigma - \diag{\mathbf s}\]
    This will ensure that, at the very least, these knockoffs are a suitable replacement for the original knockoffs for linear regression. \par
    Generating such knockoffs for a completely general case of any $\mathbf x$ is likely infeasible. However, in the specific case where $\mathbf x$ is a random binary vector variable, this paper will demonstrate a method to generate such knockoff variables. Since binary data is extremely common in many areas, this will hopefully serve as a useful tool.

\subsection{Paper Organization}
The rest of this paper will be organized in the following fashion:
\begin{itemize}
    \item Section 2 will go into more detail about how the original knockoffs breakdown with generalized linear models. In particular, several simulations will demonstrate how they breakdown.
    \item Section 3 will develop the method and theory for generating binary knockoffs.
    \item Section 4 will discuss tests of Binary knockoffs in simulation, both in comparison to the original knockoffs with Lasso and for logistic regression by itself.
    \item Section 5 is the final section, and will contain discussion of the results as well as areas for further work.
\end{itemize}

\section{Issues With Deterministic Knockoffs} 
    {\em Note:} I will try to hold to the convention that $X$ is the $n\times p$ data matrix, while $\bf x$ is the random vector variable from which each row of $X$ was drawn. Accordingly, $\tilde X$ will be the knockoff matrix while $\bf \tilde x$ is a random vector variable, at least when $\tilde X$ is generated randomly. $x_i$ will be the random variable corresponding to the $i$th entry of $\bf x$, while $X_i$ is $i$th column of $X$, with observations drawn from $x_i$. $\hat \Sigma = \frac{1}{n}X^TX$ is the empirical covariance matrix associated with $X$, while $\Sigma=\E(\mathbf x \mathbf x^T)$ is the theoretical covariance matrix associate with $\mathbf x$. Also, for a square matrix $A$, $\diag A$ is the vector of values along the diagonal, while for a vector $\mathbf a$, $\diag \mathbf a$ is a square diagonal matrix with $\mathbf a$ along the diagonal.\\

    Simulations with deterministic knockoffs reveal that they don't perform well in L1 regularized logistic regression. Even when $X_i$ is a null predictor of $y$, the $X_i$ still tend to enter the model prior to $\tilde X_i$. The issue is that even when $x\sim N_p({\bf 0}, \Sigma)$ for some $\Sigma\succeq 0$, the $\tilde X_i$ do not fit a normal distribution. This can be seen below in figure 1, where normal Q-Q plots of simulated variables $X_i$ and corresponding knockoffs $\tilde X_i$ are provided. Of course, when $X$ is a binary vector, $\tilde X$ completely doesn't match its distribution.
    \begin{figure}[h!]
        \includegraphics[width=20cm]{images/normalQQ}
        \caption{Normal Q-Q of original variables and knockoffs for simulation with $4$ variables and 1,000 observations}
    \end{figure}
    These 
    

\section{Random Bernoulli Knockoffs}
    My solution to the issues with the 

    generate $\bf \tilde x$ randomly such that it has the desired knockoff properties in expectation. In particular, both  should have similar marginal densities, expectations, and second moments. However, $\bf \tilde x\st \bf x$ should also have desired knockoff property that $\E(\mathbf{\tilde x}^T\mathbf x\st \mathbf x) = \Sigma-\mathbf s$, where $\|\diag{\Sigma}-s\|$ is small. In the general case, this is likely infeasible; however, if $\bf x$ is a binary vector, as is often the case, we know we are dealing with a much more limited class of random variables, and it should be possible to randomly generate $\bf \tilde x\st \bf x$ so as to have the desired properties. At worst, this method will provide a suitable replacement for deterministic $\bf \tilde x$ for use with LASSO, and if we are lucky, it will work reasonably for other regularized GLMs. 

\subsection{Random Bernoulli Generation}
Thankfully, there has been a reasonable amount of work on how one can generate random Bernoulli vectors with some kind of correlation among among the values. A random Bernoulli vector $\bf x$ can be summarized by its first two moments: a mean vector $\E(\bf x)=m\in(0,1)^p$ and cross-moment matrix $\E(\bf x\bf x^T)=M\in(0,1)^{p\times p}$. Obviously, $m_i = \p(x_i=1)$, $M_{ij} = \p(x_i=x_j=1)$, and $\mathbf m=\diag{M}$. For an arbitrary symmetric $M$ to be valid cross-moment matrix, $M-mm^T\succeq0$, and
    \[ \max\{0,m_i+m_j -1\} \leq M_{ij} \leq \min\{m_i,m_j\}\]
    for all $i\neq j$\footnote{``On parametric families for sampling binary data with specified mean and correlation" - http://arxiv.org/abs/1111.0576}. Given a qualifying $M$, or observed $X$, there are a few ways of generating more random $\bf x$.

\subsubsection{Gaussian Copula Family}
    Since multivariate normal distributions are easy to randomly draw, the idea is to find some random normal variable $\mathbf z\sim N_p({\bf 0},\Sigma)$ such that,for $x_i = I(z_i<0)$, $x$ has the desired properties. There are a number of ways to do this\footnote{``On the Generation of Correlated Artificial Binary Data" - http://epub.wu.ac.at/286/1/document.pdf}\footnote{``On parametric families for sampling binary data with specified mean and correlation"}, but it turns out that there is only certain to exist a working $\Sigma$ in the bivariate case. 

\subsubsection{$\mu$-Conditionals family}
    There exists a more flexible family which will always work for arbitrary $M$ called $\mu$-conditionals. The basic idea is that the $X$ is generate sequentially as 

    \[ \p(x_{i}=1\st x_{1},...,x_{i-1}) = \mu\left(a_{ii}+\sum_{k=1}^{i-1}a_{ik}x_i\right) \]

    for some monotone function $\mu:\R\to(0,1)$. This is essentially a binomial family GLM for a link function $\mu$. If one takes all of the $a_{kj}$, they can form a lower triangular matrix $A$, and then the joint density can be expressed as 

    \[ \p(\mathbf x=\mathbf \gamma) \propto \mu(\mathbf{\gamma}^T A\mathbf \gamma)\]

    If $\mu$ is chosen such that it is a bijection and differentiable, there is a unique $A$ such that $\E(\mathbf x \mathbf x^T)=M$ when generated from this model\footnote{``On parametric families for sampling binary data with specified mean and correlation"}. The natural choice for $\mu$ is the logistic link function, which yields the Ising model, the ``binary analogue of the multivariate normal distribution which is the maximum entropy distribution on $\R^p$ having a given covariance matrix." Additionally, it has the usual benefit that the coefficients can be viewed as a log odds ratio:
    \[a_{ij} = \log\left(\frac{\p(x_j=x_k=1)\p(x_j=x_k=0)}{\p(x_j=0,x_k=1)\p(x_j=1,x_k=0)}\right) \]
    when $i\neq j$. I think this dictates that if $\bf x$ is generated from this model with $a_{ij}=0$, then $x_i$ and $x_j$ are independent.  \par

    There is no closed form to calculate the entries in $A$ if $p>1$, but they can be derived numerically two ways.  
    \begin{enumerate}
        \item If one is attempting to replicate the empirical cross-moments from a data matrix $X$, $a_{1i}$ to $a_{ii}$ can be derived from fitting successive logistic regressions of $X_i$ on $X_{1} \ldots X_{i-1}$ using maximum likelihood. $a_{ji}$ for $i\neq j$ will then be the coefficient on $X_j$ while $a_{ii}$ is the intercept of the regression.
        \item If one is just working with a desired cross-moment matrix $M$, the successive rows of $A$ can be fit via Newton-Raphson. \par
            Let us say that the first $i-1$ rows have already been fit, resulting in the upper left $(i-1)\times(i-1)$ sub matrix $A_{-i}$ of $A$. Let us say that $\mathbf a_i$ is the first $i$ entries of the $i$th row of $A$ (the rest will be 0 anyway). As well, let $\mathbf m_i$ be similarly the first $i$ entries of the $i$th row of $M$. In other words, $\mathbf{m}_i = [\E(x_i x_j)]_{j=1}^{i}$. Finally, let us say that $\mathbf x_{-i}$ is the first $i-1$ entries of $\mathbf x$. We want to solve for $\mathbf a_i$ such that
            \begin{align*}
                \mathbf m_i &= \E\left(x_i \left[\begin{array}{c} \mathbf x_{-i} \\ x_i \end{array}\right]\right) \\
                \mathbf m_i &= \E\left(\E\left(x_i \left[\begin{array}{c} \mathbf x_{-i} \\ x_i \end{array}\right]\;\bigg \vert\; \mathbf x_{-i}\right)\right)\\
                \mathbf m_i &= \sum_{\mathbf x_{-i} \in \{0,1\}^{i-1}} \p(\mathbf x_{-i})\p(x_i=1\st \mathbf x_{-i}) \left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right] \\
        \mathbf m_i &= \sum_{\mathbf x_{-i} \in \{0,1\}^{i-1}} \frac{1}{c}\mu\left(\mathbf x_{-i}^TA_{-i}\mathbf x_{-i}\right) \mu\left(\mathbf{a}_i^T\left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right] \\
            \end{align*}
            Where $c$ is the appropriate normalizing constant. Let us define the quantity on the right in the last line as $f(\mathbf a_i)$. We can solve for $f(\mathbf a_i)=\mathbf{m}_i$ by successive Newton-Raphson iterations defined by 
            \[\mathbf a_i^{(k+1)} = \left[H f\left(\mathbf a_i^{(k)}\right)\right]^{-1}\left[f\left(\mathbf a_i^{(k)}\right)-\mathbf m_i\right] \]
            The Hessian matrix is calculated as 
        \[ H f\left(\mathbf a_i\right) = \sum_{\mathbf x_{-i} \in \{0,1\}^{i-1}} \frac{1}{c}\mu \left(\mathbf x_{-i}^TA_{-i}\mathbf x_{-i}\right) \mu'\left(\mathbf{a}_i^T\left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_{-i} \\ 1 \end{array}\right]\left[\begin{array}{cc} \mathbf x_{-i}^T & 1 \end{array}\right] \]
            With $2^{i-1}$ possible values for $\mathbf x_{-i}$, this can quickly become computationally expensive. Instead, with a series of values $\mathbf x_{-i}^{(k)}\sim \mathbf x_{-i}$, we can approximate
            \[f\left(\mathbf a_i\right) \approx \frac{1}{K}\sum_{k=1}^K \mu\left(\mathbf{a}_i^T\left[\begin{array}{c} \mathbf x_{-i}^{(k)} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_{-i}^{(k)} \\ 1 \end{array}\right] \]
            and
            \[H f\left(\mathbf a_i\right) \approx \frac{1}{K}\sum_{k=1}^K \mu'\left(\mathbf{a}_i^T\left[\begin{array}{c} \mathbf x_{-i}^{(k)} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_{-i}^{(k)} \\ 1 \end{array}\right]\left[\begin{array}{cc} \mathbf [x_{-i}^{(k)}]^T & 1 \end{array}\right] \]
            Though in theory $A$ should always exist, in practice numerical issues may compound to the point that the Newton-Raphson method won't converge. In this case, one can solve instead for $\mathbf m_i^*(\tau)$, where, for $\tau\in[0,1]$
    \[\mathbf m_i^*(\tau) = (1-\tau)\mathbf m_i + \tau \left[ \begin{array}{cccc} 0 & \ldots & 0 & M_{ii} \end{array} \right]^T \]
            When $\tau=0$, this yields the original problem, while when $\tau=1$, it is treating $x_i$ as independent of $\mathbf x_{-i}$. The latter will always have the solution
            \[\mathbf a_i = \left[ \begin{array}{cccc} 0 & \ldots & 0 & \log\left(\frac{M_{ii}}{1-M_{ii}}\right) \end{array} \right]^T \]
            The hope is that for some $\tau$ close to $0$, convergence can be achieved, only causing a slight distortion from the desired cross moments.
    \end{enumerate}

\subsection{Generating Binary Knockoffs}
    My method for generating binary knockoffs broadly involves two steps:
    \begin{enumerate}
        \item I use either the equal correlation method or the SDP method described in the original knockoff paper to find $\mathbf s$ such that $\|\diag{\hat\Sigma}-\mathbf s\|$ is small and 
        \[ \Sigma_L = \left[\begin{array}{cc}  \hat\Sigma & \hat\Sigma - \diag{\mathbf s }\\ \hat\Sigma - \diag{\mathbf s} & \hat\Sigma \end{array}\right] \succeq 0 \]
        I use a subscript $L$ for large to denote items associated with the joint distribution of $[\mathbf x \; \mathbf{\tilde x}]$. If $\mathbf m_L = \E\left([\mathbf x \; \mathbf{\tilde x}]\right)^T = [\mathbf m \; \mathbf m]^T$, then the desired cross moment matrix of the joint distribution is 
        \[ M_L = \Sigma_L + \mathbf m_L\mathbf m_L^T\]
        To ensure that this is a valid cross moment matrix for a binary random vector is that 
        \[\max\{0,\mathbf m_{L,i} + \mathbf m_{L,j} -1\} \leq M_{L,ij} \leq \min\{\mathbf m_{L,i}, \mathbf m_{L,j}\} \]
        I've built a check into the code for this, but in practice it shouldn't be a worry. This condition is always satisfied in a neighborhood of $M_{L,ij} = \mathbf m_{L,j}\mathbf m_{L,i}$. Since the we are either keeping the value $M_{L,ij}$ from a valid cross moment matrix or minimizing $\vert M_{L,ij} - \mathbf m_{L,j}\mathbf m_{L,i}\vert$, it would be very surprising if this condition was violated.
        
        \item I can fit the matrix $A$ that will generate random binary variables similarly to the method described in section $3.2$ which have cross moments $X_L$. This can be used to generate the $\tilde x_i$ sequentially as $\tilde x_i \st \mathbf x,\tilde x_1,\ldots,\tilde x_{i-1}$ so as to create $\tilde X \st X$. 
    \end{enumerate}
    \subsection{More Detail on Fitting $A$}
        I could fit $A$ based on $M_L$ exactly as described in the second method of $3.2$, however, this isn't exactly what I do. First off, with $p$ being potentially large, the simulation method for estimating $f(\mathbf a)$ and $H f(\mathbf a)$ was the obvious choice. This involves fitting $\mathbf a_i$ based on the conditional distribution of $\mathbf x_{L,i}$ given randomly drawn partial vectors $\mathbf x_{L,-i}$. There is no need to simulate the marginal distribution of $\mathbf x$ though, since the simulation is only approximate and we already have a number of realizations in $X$. Thus, I only fit the lower half of $A$, using this process: 
        \begin{enumerate}
            \item To get a simulation of size at least $K$, I create a matrix $X_F$ ($F$ for fixed) which is initially $X$ stacked up until it has $K'\geq K$ rows. 
            \item For each $p<i\leq 2p$,
                \begin{itemize}
                    \item Where $\mathbf x_F^{(k)}$ is the $k$th row of $X_F$, the rows $\mathbf a_i$ are fit sequentially by Newton-Raphson iterations with 
                    \[f\left(\mathbf a_i\right) \approx \frac{1}{K'}\sum_{k=1}^{K'} \mu\left(\mathbf{a}_i^T\left[\begin{array}{c} \mathbf x_F^{(k)} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_F^{(k)} \\ 1 \end{array}\right] \]
                    and
                    \[H f\left(\mathbf a_i\right) \approx \frac{1}{K'}\sum_{k=1}^{K'} \mu'\left(\mathbf{a}_i^T\left[\begin{array}{c} \mathbf x_F^{(k)} \\ 1 \end{array}\right]\right)\left[\begin{array}{c} \mathbf x_F^{(k)} \\ 1 \end{array}\right]\left[\begin{array}{cc} \mathbf [x_F^{(k)}]^T & 1 \end{array}\right] \]
                    \item If the iterations won't converge, I attempt to fit $f(\mathbf a_i)=m_i^*(\tau)$ instead for increasing values of $\tau$ until it converges.
                    \item After $\mathbf a_i$ is fit, a new column $X_i$ is drawn as independent Bernoulli with probability vector $\mu(X_F \mathbf a_i)$.   
                    \item $X_F$ is updated to 
                    \[ X_F = [X_F \; X_i] \]
                \end{itemize}
            \item At the end, the first $n$ rows of $X_F$ are taken as $[X \; \tilde X]$, though the rows corresponding to any copy of $X$ would work equally well, since these all should have the desired distribution.
        \end{enumerate}
        Some thoughts/concerns/explanations:
        \begin{itemize}
            \item Since $x_{-i}^{(k)}$ is replaced with $x_F^{(k)}$, which is a hybrid of real and simulated data, I am not sure there is the same theoretical guarantee that a unique $A$ matrix exists. In practice though it still worked pretty well, and has the advantage that we are deriving $\tilde X$ such that $\E(X'\tilde X) = \hat \Sigma- \diag{\mathbf s}$.
            \item A hybrid of simulating $x_{-i}^{(k)}$ and keeping fixed $x_F^{(k)}$ would be to draw from the rows of $X$ $K$ times, with replacement, then simulating the rest of the $x_{-i}^{(k)}$ vector. I'm not convinced there is a good reason to do this. 
            \item By not simulating, there is the advantage of not needing to fit the upper half of $A$.
            \item By not redrawing $x_{-i}^{(k)}$ each time, there is less computation for the computer. As well, the multiplication
                \[ \left[\begin{array}{c} \mathbf x_F^{(k)} \\ 1 \end{array}\right]\left[\begin{array}{cc} \mathbf [x_F^{(k)}]^T & 1 \end{array}\right] \]
            needn't be redone for each iteration, and only partially recalculated for each $i$ (though I don't have this implemented yet). The downside might be that error is getting compounded over each $i$.
        \end{itemize}

\section{Bernoulli Knockoff Performance}
For each of thses, exaimine these situatations:
\begin{itemize}
    \item $X$ arises out of the assumed Isling model, with no high order interactions. 
    \item $X$ does not arise out of this model, and does have higher order interactions. Could possibly model this by drawing the vector $\mathbf x$ or subsets of it as multinomial with probabilities from a Dirichlet distribution.
    \item $X$ is from a real world data set, likely something in genetics.
    \item $y$ is normally distributed around $X\beta$.
    \item $y$ is drawn from some other distribution, possibly skewed, heavy tailed, or light tailed.
    \item $y$ is from a real world data set.
\end{itemize}
\subsection{Gram Matrix Comparison}
\subsection{Binary vs. Determinist Knockoffs in LASSO}
\subsection{Binary Knockoffs in other Regularlized GLMS}

\section{Discussion}
\subsection{Areas for Further Work}


\section{Further Work and Simulations}
As I see it, further simulation work for my paper breaks down into three logical groups: comparison of random Bernoulli knockoffs to the original Knockoffs in LASSO, evaluation of Bernoulli knockoffs in other L1 regularized GLMs, and expanding random knockoffs to more general sorts of variables.

\subsection{Comparison to Original Knockoffs in LASSO}
The basic idea is to compare how the two sorts of knockoffs compare when $X$ is binary and we are fitting a LASSO regression. Do they select the same variables? Do the binary knockoffs control FDR more or less conservatively? Situations to test:
\begin{itemize}
    \item $X$ arises out of the assumed Isling model, with no high order interactions. 
    \item $X$ does not arise out of this model, and does have higher order interactions. Could possibly model this by drawing the vector $\mathbf x$ or subsets of it as multinomial with probabilities from a Dirichlet distribution.
    \item $X$ is from a real world data set, likely something in genetics.
    \item $y$ is normally distributed around $X\beta$.
    \item $y$ is drawn from some other distribution, possibly skewed, heavy tailed, or light tailed.
    \item $y$ is from a real world data set.
\end{itemize}

\subsection{Evaluation of Binary Knockoffs in other GLMS}
The one of the most interest would be logits. It would be good to see how successful the binary knockoffs are in controlling FDR without the theoretical guarantees that LASSO provides.
\begin{itemize}
    \item $X$ arises out of the Isling model, in which case it seems like the knockoffs should control FDR.
    \item $X$ has higher order interactions, which might make the knockoffs perform poorly.
    \item $X$ is from a real world data set, likely something in genetics.
    \item $y$ is simulated based on the assumptions of the regression model.
    \item $y$ is simulated to violate assumptions of the regression model.
    \item $y$ is from a real world data set.
\end{itemize}

\subsection{Generalizations/Harebrained Ideas}
I have two seeds of ideas for extensions if I have enough time.
\begin{enumerate}
    \item Still with binary data, one might be able to simulate binary variables with higher order interactions in the generation of $X$ by including higher order interactions in the regression of $X_i$ on $X_{-i}$. This would lead to $A$ being a matrix of higher dimension. Even if this worked, extending the method to knockoffs may not be obvious.
    \item In the general case where $X$ is not binary, I can almost imagine a method set up along similar lines to the binary case.
    \begin{itemize}
        \item The desired covariance matrix could be chosen as in the original knockoff paper.
        \item Each $x_i$'s marginal distribution would be approximated by a kernel density estimate on $X_i$.
        \item A $x_i \st x_1, \ldots, x_{i-1}$ would be drawn from some reweighing of this marginal to achieve the proper covariance. For instance, if $F_i^{-1}$ is the inverse CDF of the marginal kernel density for $x_i$ and $u_i\st x_1, \ldots, x_{i-1}$ is a RV on $(0,1)$, then $x_i=F_i^{-1}(u_i)$.
        \item Maybe fit generalized additive model for each successive $x_i$ with kernel regression for $x_1,\ldots,x_{i-1}$ to predict mean. Then, skew marginal of $x_i$ until it has that mean. 
    \end{itemize}
\end{enumerate}



    

\end{document}
