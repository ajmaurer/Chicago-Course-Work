{"nbformat_minor": 0, "cells": [{"execution_count": 1, "cell_type": "code", "source": "# Imports\nimport json, re, numpy as np, numpy.linalg as nplin, matplotlib.pyplot as plt, matplotlib.mlab as mlab, scipy.stats as spstat\nfrom __future__ import division\nfrom pyspark.mllib.feature import HashingTF, IDF, Normalizer\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import DecisionTree, RandomForest, GradientBoostedTrees\nfrom operator import itemgetter\nfrom time import time\n%matplotlib inline", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 2, "cell_type": "code", "source": "# Load data, split into test and training set\nall_reviews = sc.textFile(\"s3n://stat-37601/ratings.json\", minPartitions=1000).map(json.loads)\nreviews, reviews_test = all_reviews.randomSplit([.7, .3])\nreviews.cache()", "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "PythonRDD[2] at RDD at PythonRDD.scala:42"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Get the variable we are regressing on: a continious score out of 1", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "# Get the variable we are regressing on, the review as a score out of 1\ndef getLabel(review):\n    \"\"\"Get the overall rating from a review\"\"\"\n    label, total = review[\"review_overall\"].split(\"/\")\n    return float(label) / float(total)\n\nlabels      = reviews.map(getLabel)\nlabels_test = reviews_test.map(getLabel)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "#### (a) Generating features (hashed TF-IDF)\nWe generate a list of words (the features) for each review, transform them into hashes, calculate term frequency-inverse document frequency accross corpus, and normalize", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "# Parser, mostly from earlier problem using SGD on tweets, except without code for emoticions\n\n# words to ignore\nstop = set(['the', 'and', 'you', 'your', 'for', 'por', 'que', 'las', 'los', 'les',\\\n       'una', 'del', 'este', 'usted', 'para', 'con', 'this', 'that', 'was', 'have', 'like',\\\n       'would', 'could', 'should', 'will', 'can', 'shall', 'just', 'all', 'it', 'its', 'per'])\neng_stop = set(['i', 'me', 'my', 'myself', 'we', 'our', \\\n             'ours', 'ourselves', 'you', 'your', 'yours', \\\n             'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', \\\n             'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', \\\n             'themselves', 'what', 'which', 'who', 'whom', 'this', \\\n             'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', \\\n             'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\\\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', \\\n             'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\\\n             'by', 'for', 'with', 'about', 'against', 'between', 'into', \\\n             'through', 'during', 'before', 'after', \\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in',\\\n            'out', 'on', 'off', 'over', 'under', 'again', 'further', \\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', \\\n            'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\\\n            'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', \\\n            'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', \\\n            'just', 'dont', 'should', 'now','on'])\nspa_stop = set()\nall_stop = stop|eng_stop|spa_stop\n\n# word processor function\ndef splitter(s,ignore=all_stop):\n    s = re.sub(\"([a-zA-Z])'([a-zA-Z])\",\"\\g<1>\\g<2>\",s) # standardize to no apostrophe\n    s = re.sub('[^a-zA-Z!\\?]',' ',s)           # get rid of most punctuation \n    s = re.sub('\\?![\\?!]*|!\\?[\\?!]*',' !? ',s) # standardize ?!?!?!\n    s = re.sub('!+','!',s)                    # standardize to single !\n    s = re.sub('\\?+','?',s)                   # standarize to single ?\n    s = re.sub('([a-zA-z]{2,})([?!]+)(\\s|$)','\\g<1> \\g<2> ',s) # single out punctuation\n    s = re.sub('(?!http://)www\\.\\S+|http://\\S+','',s) # get rid of urls\n    return list([w.lower() for w in s.split() if w not in ignore])", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "I only take 200 features for the hashing space. This means there are going to be a huge number of collisions, resulting in a large but unintelligent dimmensionality reduction. The computational time and space issues become prohibitive without a large reduction though. We still end up with some predictive power anyway though, so we aren't totally ruining the regression.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "# Do the hashing transform.\nrevHTF = HashingTF(numFeatures=200)\nreviewFrequency      = revHTF.transform(     reviews.map(lambda review: splitter(review[\"review_text\"]))).cache()\nreview_testFrequency = revHTF.transform(reviews_test.map(lambda review: splitter(review[\"review_text\"]))).cache()\n\n# Do the inverse document frequency transform\nrevIDF = IDF().fit(reviewFrequency)\nnor = Normalizer(p=2)\nfeatures      = nor.transform(revIDF.transform(reviewFrequency)).cache()\nfeatures_test = nor.transform(revIDF.transform(review_testFrequency)).cache()\n\n# Un-cache unneeded data sets\nreviewFrequency.unpersist()\nreview_testFrequency.unpersist()", "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "PythonRDD[4] at RDD at PythonRDD.scala:42"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 6, "cell_type": "code", "source": "# Join the labels back with the features\ndata = features.zip(labels).map(lambda (feature, label): LabeledPoint(label, feature)).cache()", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "Look at a 1000 feature hasing space too.", "cell_type": "markdown", "metadata": {}}, {"source": "#### (b) The regression model and evaluation\nWrite a function to compute MSE for training and test datasets", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "def treeMSE(tree,train_feat=features,train_label=labels,test_feat=features_test,test_label=labels_test):\n    '''\n    Evaluates training and test error for a pyspark mllib tree model\n    '''\n    train_MSE = train_label.zip(tree.predict(train_feat)).map(lambda (l,p):(l-p)**2).sum() / train_label.count()\n    test_MSE  = test_label.zip(tree.predict(test_feat)).map(lambda (l,p):(l-p)**2).sum() / test_label.count()\n    return(train_MSE,test_MSE)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "####(c) train and test some trees\nFirst, random forrests", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "model=RandomForest.trainRegressor(data=data,categoricalFeaturesInfo={},numTrees=2,impurity='variance',maxDepth=4,maxBins=25)\ntrerr,teerr = treeMSE(model)\nprint 'With %d trees, got %.4f training error MSE and %.4f testing error' % (2,trerr,teerr)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "With 2 trees, got 0.0252 training error MSE and 0.0252 testing error\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 42, "cell_type": "code", "source": "# Try with max depth 4 and max bins 25\n# Suprisingly, I don't see a way to add additional trees to a random forrest once its initially fit, so I'm\n# averaging accross the forrests on my own to save computation\nmarg_trees = [10,15,25,50]\nmodel_set1 = []\nct=0\nfor i in range(len(marg_trees)):\n    model_set1.append(RandomForest.trainRegressor(data=data,categoricalFeaturesInfo={},numTrees=marg_trees[i],impurity='variance',maxDepth=4,maxBins=25))\n    if i==0:\n        train_fit = labels.zip(model_set1[i].predict(features))\n        test_fit  = labels_test.zip(model_set1[i].predict(features_test))\n    else:\n        cw        =            ct/(ct+marg_trees[i])\n        mw        = marg_trees[i]/(ct+marg_trees[i])\n        train_fit = train_fit.zip(model_set1[i].predict(features)     ).map(lambda ((l,cp),mp): (l,cw*cp+mw*mp) )\n        test_fit  =  test_fit.zip(model_set1[i].predict(features_test)).map(lambda ((l,cp),mp): (l,cw*cp+mw*mp) )\n    ct += marg_trees[i]\n    # Training then test error\n    trerr = train_fit.map(lambda (l,p):(l-p)**2).sum()/train_fit.count()\n    teerr = test_fit.map(lambda (l,p):(l-p)**2).sum()/test_fit.count()\n    print 'With %d trees, got %.4f training error MSE and %.4f testing error' % (ct,trerr,teerr)\n    ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "With 10 trees, got 0.0247 training error MSE and 0.0247 testing error\nWith 25 trees, got 0.0246 training error MSE and 0.0246 testing error\nWith 50 trees, got 0.0246 training error MSE and 0.0246 testing error\nWith 100 trees, got 0.0246 training error MSE and 0.0246 testing error\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "It doesn't seem to change much even after only 10 trees, probably because it's relatively shallow and on highly reduced data. Lets see if ramping up the complexity improves things. Also, the testing and training error only appear to be the same due to the rounding, they are actually different.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": "# Max depth 6, max bins 50, 25 trees\nmodel=RandomForest.trainRegressor(data=data,categoricalFeaturesInfo={},numTrees=25,impurity='variance',maxDepth=6,maxBins=50)\ntrerr,teerr = treeMSE(model)\nprint '%.4f training error MSE and %.4f testing error' % (trerr,teerr)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.0239 training error MSE and 0.0239 testing error\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Now some gradient boosted trees. Start by looking out how error changes over number of iterations.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 52, "cell_type": "code", "source": "# Least squares loss, max depth of 1\nmodel_set2 = []\nfor itnum in [10,50,100]:\n    model_set2.append(GradientBoostedTrees.trainRegressor(data=data,categoricalFeaturesInfo={},numIterations=itnum,loss='leastSquaresError',maxDepth=1))\n    trerr,teerr = treeMSE(model_set2[-1])\n    print 'With %d iterations, got %.4f training error MSE and %.4f testing error' % (itnum,trerr,teerr)", "outputs": [{"ename": "KeyboardInterrupt", "evalue": "", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "\u001b[1;32m<ipython-input-52-cb31b92f0fab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_set2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitnum\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel_set2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGradientBoostedTrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategoricalFeaturesInfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumIterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'leastSquaresError'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxDepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtrerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mteerr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtreeMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_set2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'With %d iterations, got %.4f training error MSE and %.4f testing error'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mitnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mteerr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/root/spark/python/pyspark/mllib/tree.py\u001b[0m in \u001b[0;36mtrainRegressor\u001b[1;34m(cls, data, categoricalFeaturesInfo, loss, numIterations, learningRate, maxDepth)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \"\"\"\n\u001b[0;32m    558\u001b[0m         return cls._train(data, \"regression\", categoricalFeaturesInfo,\n\u001b[1;32m--> 559\u001b[1;33m                           loss, numIterations, learningRate, maxDepth)\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/root/spark/python/pyspark/mllib/tree.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(cls, data, algo, categoricalFeaturesInfo, loss, numIterations, learningRate, maxDepth)\u001b[0m\n\u001b[0;32m    439\u001b[0m     def _train(cls, data, algo, categoricalFeaturesInfo,\n\u001b[0;32m    440\u001b[0m                loss, numIterations, learningRate, maxDepth):\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"the data should be RDD of LabeledPoint\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         model = callMLlibFunc(\"trainGradientBoostedTreesModel\", data, algo, categoricalFeaturesInfo,\n", "\u001b[1;32m/root/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1241\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \"\"\"\n\u001b[1;32m-> 1243\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/root/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/root/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m         \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjavaPartitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowLocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m    538\u001b[0m                 self.target_id, self.name)\n", "\u001b[1;32m/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/lib64/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    428\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;31mKeyboardInterrupt\u001b[0m: "], "output_type": "error"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}