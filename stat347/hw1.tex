
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
STAT347 Homework 1 \hfill Aaron Maurer
\vspace{2mm}
\hrule
\vspace{2mm}

\begin{itemize}
    \item[1.]
        \begin{itemize}
            \item[(a)] One would usually want to measure covariates before assigning treatment so that treatments could be assigned on the basis of covariates. By assigning each treatment to a variety of values of covariates (for instance, each unit in a block receiving different treatment), it becomes less likely the effect of the covariate is confounded with the effect of the treatment.
            \item[(b)] Let $v$ be an additional unit such that, where $S$ is the set of all units, \(\forall v' \in S\backslash \{v\}, R(v,v')\neq1\). Since we can not apply the given logic to $v$, it need not be the case \(R(v,v)=1\), so the relation is not necessarily reflexive. 
            \item[(c)] Site, race, sex, and martial status are unordered qualitative factors, since they each take on a finite number of levels which admit not obvious ordering. I would classify age as a quantitative covariate, since it can be measured as a continuous (as opposed to discrete) value; in practice however, it can also be treated as an ordered qualitative factor, since it is usually recorded in discrete year values.
            \item[(d)]
                \begin{itemize}
                    \item[(i)]
                        For the \(y\sim A + {\bf 1}\) model, the model matrix is
                        \[ \left[ \begin{array}{cccc} 1 & 0 & 0 & 1 \\
                                                      1 & 0 & 1 & 0 \\
                                                      1 & 0 & 1 & 0 \\
                                                      1 & 0 & 0 & 1 \\
                                                      1 & 1 & 0 & 0 \\
                                                      1 & 0 & 1 & 0 \\
                                                      1 & 0 & 0 & 1 \\
                                                      1 & 1 & 0 & 0 \\
                                                      1 & 1 & 0 & 0 \\
                                    \end{array} \right] \]
                                    The second column, corresponding to level 1 of A, would be omitted under the default in R (treatment constraint). For the \(y\sim A - {\bf 1}\) model, the model matrix is
                        \[ \left[ \begin{array}{cccc} -1 & 0 & 0 & 1 \\
                                                      -1 & 0 & 1 & 0 \\
                                                      -1 & 0 & 1 & 0 \\
                                                      -1 & 0 & 0 & 1 \\
                                                      -1 & 1 & 0 & 0 \\
                                                      -1 & 0 & 1 & 0 \\
                                                      -1 & 0 & 0 & 1 \\
                                                      -1 & 1 & 0 & 0 \\
                                                      -1 & 1 & 0 & 0 \\
                                    \end{array} \right] \]
                        Once again, the second column would be omitted. These two models are equivalent in the sense they would yield the same predictions. This is because they define the same subspace since \({\bf -1} \in span({\bf 1})\). For \(y \sim A + B\), the model matrix would be
                        \[ \left[ \begin{array}{cccccc}
                                                      0 & 0 & 1 & 1 & 0 & 0 \\
                                                      0 & 1 & 0 & 1 & 0 & 0 \\
                                                      0 & 1 & 0 & 1 & 0 & 0 \\
                                                      0 & 0 & 1 & 0 & 1 & 0 \\
                                                      1 & 0 & 0 & 0 & 1 & 0 \\
                                                      0 & 1 & 0 & 0 & 1 & 0 \\
                                                      0 & 0 & 1 & 0 & 0 & 1 \\
                                                      1 & 0 & 0 & 0 & 0 & 1 \\
                                                      1 & 0 & 0 & 0 & 0 & 1 \\
                                    \end{array} \right] \]
                        Here, the 4th column, corresponding to the $1$ level of B would be omitted, since it is in the span of the rest of the columns.
                    \item[(ii)]
                        If we treat A as a quantitative covariate, then for the \(y\sim A + {\bf 1}\) model, the model matrix is
                        \[ \left[ \begin{array}{cc} 1 & 3 \\
                                                    1 & 2 \\
                                                    1 & 2 \\
                                                    1 & 3 \\
                                                    1 & 1 \\
                                                    1 & 2 \\
                                                    1 & 3 \\
                                                    1 & 1 \\
                                                    1 & 1 \\
                                    \end{array} \right]
                        \]
                        With no columns omitted in R. For the \(y\sim A - {\bf 1}\) model, the model matrix is
                        \[ \left[ \begin{array}{cc}   -1 & 3 \\
                                                      -1 & 2 \\
                                                      -1 & 2 \\
                                                      -1 & 3 \\
                                                      -1 & 1 \\
                                                      -1 & 2 \\
                                                      -1 & 3 \\
                                                      -1 & 1 \\
                                                      -1 & 1 \\
                                    \end{array} \right]
                        \]
                        With no columns omitted in R either. This model still defines an equivalent linear space to the \(y\sim A + {\bf 1}\) model, so would yield identical predictions. Finally, For \(y \sim A + B\), the model matrix would be
                        \[ \left[ \begin{array}{cccc}
                                                      3 & 1 & 0 & 0 \\
                                                      2 & 1 & 0 & 0 \\
                                                      2 & 1 & 0 & 0 \\
                                                      3 & 0 & 1 & 0 \\
                                                      1 & 0 & 1 & 0 \\
                                                      2 & 0 & 1 & 0 \\
                                                      3 & 0 & 0 & 1 \\
                                                      1 & 0 & 0 & 1 \\
                                                      1 & 0 & 0 & 1 \\
                                    \end{array} \right] 
                        \]
                        Which would have no columns omitted.
                    \item[(iii)]
                        For each of these subspaces, the dimension is the same as the dimension of the model matrix as created above. The squared length of the orthogonal projection is, when the model matrix is $X$, \(\|X(X'X)^{-1}X'y\|^2\) and the squared length of the orthogonal complement is \(\|(I-X(X'X)^{-1}X')y\|^2\). This yields:
                        \FloatBarrier
                        \input{hw1/pr1_d_iii}
                        \FloatBarrier
                    \item[(iv)] I would use the sum of squares from A. Specifically, I would compare the marginal difference in sum of squares between the projection onto the A space and onto the ${\bf 1}$ space, and compare this to the sum of squares in the complement space to A. It appears B has little if any effect, so it can probably be safely ignored for this purpose. If there is no treatment effect, these two values should both be $\chi^2$ variables with the respective degrees of freedom, so we can use them to construct a F test:
                        \[ \frac{\frac{330.52-258.89}{3-1}}{\frac{13.99}{6}} = 15.361\]
                        Checking against a $F_{2,6}$ distribution, this corresponds to a p-value of $.0044$.
                \end{itemize}
        \end{itemize}
    \item[2.]
        \begin{itemize}
            \item[(i)]
                This table can be gotten directly from the anova command in R
                \FloatBarrier
                \input{hw1/pr2_i}
                \FloatBarrier
            \item[(ii)]
                This table is a bit more complicated. I generated most of the output from a series of anova commands (Listing the spaces in increasing size so as to get the right quotient spaces), but then recalculated the F and p-values based on the overall residuals from the first model:
                \FloatBarrier
                \input{hw1/pr2_ii}
                \FloatBarrier
            \item[(iii)] This table strongly indicates that temperature an effect on fungal growth, since temperature as a categorical variable has a very low p-value on it. However, lower order polynomials seem to be sufficient to capture the effect; while the linear fit is poor, adding both a 2nd and 3rd degree polynomial seems to cause statistically significant improved fits. After this though, greater complexity does not seem to cause statistically significant improvement, indicating a 3rd degree polynomial is optimal.
            \item[(iv)]
                I've plotted the cubic fit below:
                \FloatBarrier
                \begin{center}
                    \includegraphics[width=9cm]{hw1/pr2_iv}
                \end{center}
                \FloatBarrier
                Based off this fit, one would predict growth rate will be maximized by a temperature of approximately 71.75.

        \end{itemize}
    \item[3.]
        \begin{itemize}
            \item[(i)]
                For each conservative matrix $V$, it must be the case $\forall i,j$ that \(V(i_0,i_1)+V(i_1,i_0)=0\Longrightarrow V(i_0,i_1)=-V(i_1,i_0) \). This means that $V$ is skew-symmetric. On the other hand, for 
                \[ W = \left[ \begin{array}{ccc} 0 & -4 & -1 \\ 4 & 0 & -2 \\ 1 & 2 & 0 \end{array} \right] \]  
                We have that \(W(3,1)+W(1,2)+W(2,3)= 1 -4 -2 \neq 0\), so not all skew-symmetric matrices are conservative. \\
                \vspace{5mm} \\
                If a matrix $V$ is additive, for a path of length $m$ where $i_m=i_0$,
                \[ \sum_{j=0}^{m-1} V(i_j,i_{j+1}) = \alpha_{i_0} + \sum_{j=1}^{m-1}(-\alpha_{i_j}+\alpha_{i_j}) - \alpha_{i_0} = 0 \]
                So $V$ is conservative. We may also conclude the converse by the following logic on any conservative matrix $V$:
                \begin{align*}
                    0 &= V(u_1,u_2) + V(u_2,u_3) + V(u_3,u_4) +V(u_4,u_1) \\
                    \Longrightarrow -V(u_4,u_1) - V(u_2,u_3) &= V(u_1,u_2) + V(u_3,u_4) \\
                    \Longrightarrow \;\;\; V(u_1,u_4) + V(u_3,u_2) &= V(u_1,u_2) + V(u_3,u_4) \\
                \end{align*}
                This is only possible if, for all $u_i, u_j$, $V(u_i,u_j)=f(u_i)+g(u_j)$ for some functions $f, g$. Since 
                \begin{align*}
                    V(u_1,u_3) &= V(u_1,u_2) + V(u_2,u_3) \\
                    f(u_1) + g(u_3) &= f(u_1) + g(u_2) + f(u_2) + g(u_3) \\
                    f(u_2) &= -g(u_2) \\
                \end{align*}
                Proving that $V(u_i,u_j) = f(u_i) - f(u_j)$ and thus that $V$ is additive. \\
                \vspace{5mm} \\
                Finally, the space of $6\times 6$ conservative matrices had dimension $5$. This is because the matrix is minimally defined by \(\{V(1,2),V(1,3),V(1,4),V(1,5),V(1,6)\}\). All the remaining entries can be calculated as a linear combination of these five values: \(V(u,j) = V(u,1)+V(1,j) = -V(1,u)+V(1,j)\). No fewer values will suffice for this purpose. 
            \item[ii]
                We can set this up as a typical least squares problem by transforming the $k\times k$ matrix $Y$ into a $k^2\times 1$ vector $y$. Then, if $R$ is the usual matrix of 1s and 0s for the row as a categorical variable, and $C$ is the same for the column, our model matrix is $X=R-C$. This gives us \(E(y)=X\alpha\). After omitting one of the columns of $X$ due to collinearity, we can then produce the normal least squares estimate $\alpha=(X'X)^{-1}X'y$. \(\alpha_i-\alpha_j = (\alpha_i+c) - (\alpha_j+c)\), so \(\{\alpha_1,\alpha_2,\alpha_3,\alpha_4,\alpha_5\}\) and \(\{\alpha_1+c,\alpha_2+c,\alpha_3+c,\alpha_4+c,\alpha_5+c\}\) are equivalent parameters, demonstrating that one parameter needs to be omitted.\\
                \vspace{5mm} \\
                When we preform this procedure on the data, we get these predictions for the $\alpha$ by electrolyte:
                \FloatBarrier
                \input{hw1/pr3_ii}
                \FloatBarrier
                As noted, the fifth value is a free parameter and was omitted.
            \item[iii]
                These two models are one just including the $\alpha$, and one where the $\alpha$ are interacted with electrolyte. Running these two models and comparing them with ANOVA, we get:
                \FloatBarrier
                \input{hw1/pr3_iii}
                \FloatBarrier
                By the interaction, we don't see quite enough of an improvement to push past the $.05$ p-value threshold, though it is a borderline case.
            \item[iv]
                Our data set only has factor covariates, so no monotone transformation will have an effect on the predictions, and non-monotone transformation wouldn't make sense. If there was evidence of a lack of normality or non-constant variance, a transformation might improve our statistical inference, but checking various residual plots leaves little indication that this is necessary.
        \end{itemize}
    \item[5.]
        This can be formulated as a linear model after transformation: 
        \[ log(Y) ~ \alpha - \beta_T t\] 
        If we are trying to model the expected half-life $h$ for a given sample under this model, we can formulate the problem as
        \begin{align*}
            log(.5) &= \hat\beta_Th \\
            \frac{1}{h} &= \frac{\hat\beta_T}{log(.5)}\\
        \end{align*}
        The value on the right is t-distributed under the assumptions of a normal model, so we can generate a $95\%$ confidence interval by $\hat\beta_T\pm t_{n-p}^{.025}se(\hat\beta)$. Then, we can take the inverse of each boundary to get a confidence interval on the $h$. We must keep in mind though that any value that is $0$ or negative before taking the inverse corresponds to a lack of decay and asorbic acid levels not actually falling. This yields the following confidence intervals for half-life (in weeks):
        \[ \begin{array}{cccc} Temperature & Lower Bound & Estimate & Upper Bound \\
                              \hline  
                              0           & 67.63       & 948.55   & \infty      \\
                              10          & 20.70       & 28.92    &  47.99       \\
                              20          & 4.89        & 5.24     & 5.65        \\
                      \end{array} \]
        It is worth noting that this does not include any prediction error. However, that error's contribution would be relatively small. This model accounts for for the variance among replicates; that variance contributes to the variance in the mean which we are fitting.


        
\end{itemize}
\end{document}
