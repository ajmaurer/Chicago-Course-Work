
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\p}{\mathrm{P}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newcommand{\hwhead}[1]{#1 \hfill Aaron Maurer \vspace{2mm} \hrule \vspace{2mm}}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hwhead{STAT 347 Homework 3, Part 1}
\begin{itemize}
    \item[0.]
        \begin{itemize}
            \item[(i)]
                We can calculate the joint distribution easily from the description:
                \[\p(Y,N) = \begin{cases} 
                              \;            \frac{1}{3} &\mbox{if } Y=0,N=0 \\
                              \;   \frac{1}{3}(1-\pi_1) &\mbox{if } Y=0,N=1 \\
                              \;       \frac{1}{3}\pi_1 &\mbox{if } Y=1,N=1 \\
                              \;   \frac{1}{3}(1-\pi_2) &\mbox{if } Y=0,N=2 \\
                              \;       \frac{1}{3}\pi_2 &\mbox{if } Y=2,N=2 \\
                              \;            0           &\mbox{otherwise } \\
                            \end{cases} \]
            \item[(ii)]
                $Y/N \st N=1$ and $Y/N \st N=2$ are both Bernoulli random variables with probability with probabilities $\pi_1$, $\pi_2$, so 
                \[\E(Y/N \st N=1) = \pi_1,\; \var(Y/N \st N=1) = \pi_1(1-\pi_1),\;\E(Y/N \st N=2) = \pi_2,\; \var(Y/N \st N=1) = \pi_2(1-\pi_2) \]
                If $\pi_1=\pi_2$, $Y/N \st N=1 \sim Y/N \st N=2$, so $Y/N$ and $N$ are uncorrelated.
            \item[(iii)]
                Since the wells are independent, $Y\sim B(26,\pi_1)+2\times B(15,\pi_2)$, so
                \[ \E[Y] = \E[B(26,\pi_1)] + 2\E[B(15,\pi_2)] = 26\pi_1 + 30\pi_2\]
                and
                \[ \var(Y) = \var[B(26,\pi_1)] + 4 \var[B(15,\pi_2)] = 26\pi_1(1-\pi_1) + 60\pi_2(1-\pi_2)\]
            \item[(iv)]
                Fitting this distribution $F$ to the data, with $m=26\times1 + 15\times 2 = 56$, we would estimate 
                \[ \mu = m\pi = \E[Y] = 26\pi_1 + 30\pi_2 \Longrightarrow \pi = \frac{26\pi_1 + 30\pi_2}{56}\] 
                so we would calculate $\sigma^2$ as
                \begin{align*}
                    \sigma^2m\pi(1-\pi) &= \var[y] \\
                    \sigma^256\frac{26\pi_1 + 30\pi_2}{56}\left(1-\frac{26\pi_1 + 30\pi_2}{56}\right) &= 26\pi_1(1-\pi_1) + 60\pi_2(1-\pi_2)\\
                    \sigma^2&= \frac{56(26\pi_1(1-\pi_1) + 60\pi_2(1-\pi_2))}{(26\pi_1 + 30\pi_2)(56-26\pi_1 - 30\pi_2)}\\
                \end{align*}
                Which, in the case where $\pi_1=\pi_2=\pi$, reduces to 
                \[ \sigma^2 = \frac{76}{56} = \frac{19}{14}\]
            \item[(v)]
                Let $Y_1$ be the number of homogamic matings in single-mating wells out of $M_1$ total,  and $Y_2$ and $M_2$ the same for two mating wells. If we assume that $\pi_1=\pi_2=\pi$, then 
                \[ Y_2/2 \sim B(M_2/2,\pi) \quad \&\quad Y_1 \sim B(M_1,\pi)\]
                so 
                \[\var(Y_2) = \var(2B(M_2/2,\pi)) = 2M_2\pi(1-\pi)\]
                and 
                \[\var(Y_1) = \var(B(M_1,\pi)) = M_1\pi(1-\pi)\]
                so, if $2M_1=2M_2=M$,
                \begin{align*}
                    \var(Y) &= 2M_2\pi(1-\pi)+M_1\pi(1-\pi) \\
                    \sigma^2M\pi(1-\pi) &= \frac{3}{2}\pi(1-\pi) \\
                    \sigma^2 &= \frac{3}{2} \\
                \end{align*}
            \item[(vi)]
                This mechanism can't explain the chi-square test, which indicated under dispersion. If it was the case that one couldn't see one homogamic and one heterogamic mating in one well as we simulated, then the result is higher variance than the multinomial model anticipates. This is the opposite of what is observed in the actual data set, suggesting this mechanism is not the explanation.  
            \item[(vii)]
                They would have concluded that the data does not follow a multinomial distribution by looking at the variance of frequencies among replicated experiments, for instance through the same chi square test as above. Since these frequencies had much lower variance than one would anticipate with a multinomial model, they concluded the frequencies weren't multinomial. This would also stand to reason, since with only two flies of each gender in a well, one pair mating would necessarily effect the probability of the other pair mating, invalidating independence among the mating events. The data not being multinomial would invalidate the standard errors and p-values, which were made under an assumed multinomial mode.
            \item[(viii)] By only analyzing the first mating in each well, they were assuring there was no dependence among mating events they counted. Since there is independence between wells, this first mating must necessarily be independent of the other counted matings. With this fix, the multinomial model should hold again, allowing for simpler inference. 
            \item[(ix)]
                It appears counting just the first mating solves the dispersion problem. When performing a Chi-Square test for the presence of mating pair-generation effects, we now get a p-value of $.107$. If we then combine the counts for all homogamic and heterogamic matings in each generation and repeat the test, we get a p-value of .785. This suggests that we have the proper dispersion for a multinomial and that generations don't have an effect on mating preference. \vspace{2mm}
                
                If subsequent matings had been analyzed, aside from the mentioned statistical issues due to dependence, the conclusion would likely have been less well supported. First, we can imagine in general, the first mating to occur is in general the one representing the strongest preference. Even if this is not the case though, due to female flies refractory period, after one mating both male flies will likely attempt to mate with the remaining female fly. One can imagine that the desire to mate is stronger than a particular preference if there is only one option, making a preference harder to discern.
            \item[(x)]
                Testing the significance of homogamic versus heterogamic pairings, I ran a logistic regression on the counts:
                \FloatBarrier
                \input{hw3/A0_ix_logit}
                \FloatBarrier
                There is extremely high significance on the intercept, strongly supporting that the probability of the first mating being homogamic is high. This supports the authors' conclusion that homogamic prefernce is preferred, though one could imagine a case where preference changes after some exposure time and an initial mating. Also, I included a second degree polynomial on generation as a second test for whether it effected mating preference; as we can see, it does not.

        \end{itemize}
    \item[3.]
        \begin{itemize}
            \item[(a)]
                These are the parameter estimates from the main-effects linear logistic model:
                \FloatBarrier
                \input{hw3/A3_a_logit}
                \FloatBarrier
                For the deviance, the residual degrees of freedom are calculated as the total number of Bernoulli trials represented in the data set minus the minus dimension of the linear space of predictors. Another way of saying this is the sum of all positive and negative outcomes minus the number of variables included in the model (after removing linearly dependent variables). 
            \item[(b)]
                With an estimate of $.1239$ on sex2, the model is estimating a small increase in the rate of affected for women, though it is not statistically significant. We can estimate a confidence interval for the odds ratio by calculating a confidence interval for the parameter based on asymptotic normality and transforming it. To start, a $90\%$ CI for the parameter is:
                \[(.1239+z_{.05}.2288,1.239+z_{.95}.2288)\quad=\quad(-.2524,.5002)\]
                Then, we transform this by exponentiating it to yields an odds ratio CI:
                \[(e^{-.2524},e^{.5002})\quad=\quad (.7769,1.6491)\]
                Thus, we can say with $90\%$ confidence that the odds ratio for women as compared to men is between $.7769$ and $1.6491$.
            \item[(c)]
                I dropped sex first, and then race from the original model to get the model below. I didn't drop dust2 because thought it wasn't significant, it was part of a factor which was extremely significant.
                \FloatBarrier
                \input{hw3/A3_c_logit}
                \FloatBarrier
                Each of the remaining factors is estimated to cause a statistically significant increase in the risk of byssinosis. The third level of dust is estimated to cause a far bigger increase in risk than any of the other variables, causing a large increase over the first and second level, which are not statistically significantly different. The smoking level 2 and employment levels 2 and 3 both are estimated to increase the risk of the disease over the first level of each.

        \end{itemize}
\end{itemize}

\end{document}
