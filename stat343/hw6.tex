
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%% AMS PACKAGES - Chances are you will want some or all of these if writing a math dissertation.
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, enumerate, multicol, graphicx, listings}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
STAT 343, Homework 6 \hfill Aaron Maurer
\vspace{2mm}
\hrule
\vspace{2mm}
\begin{itemize}
    \item[1.] Problem 5.3 in Weisberg \\
        I got an estimate of \(\hat\sigma^2=1860.276\), so for the test statistic I got that \(G=\frac{\sum_{i=1}^n(\hat y_i - \tilde y_i)^2}{\hat\sigma^2}=15.296\). Then, running the bootstrap algorithm with $1000$ bootstraps, I got a significance level of  $.002$, since only $\frac{1}{1000}$ of the bootstrap samples resulted in a higher test statistic.
    \item[2.]
        \begin{itemize}
            \item[a)]
                \begin{align*}
                    X_{(i)}^TX_{(i)}&=\sum_{j\neq i} x_j^Tx_j \\
                    X_{(i)}^TX_{(i)}&= X^TX - x_i^Tx_i \\
                    (X_{(i)}^TX_{(i)})^{-1}&= (X^TX - x_i^Tx_i)^{-1} \\
                \end{align*}
                Applying the Sherman-Morrison formula to the right hand side, we get
                \begin{align*}
                    (X_{(i)}^TX_{(i)})^{-1}&= (X^TX)^{-1} + \frac{(X^TX)^{-1}x_i^Tx_i(X^TX)^{-1}}{1-x_i(X^TX)^{-1}x_i^T} \\
                    (X_{(i)}^TX_{(i)})^{-1}&= (X^TX)^{-1} + \frac{(X^TX)^{-1}x_i^Tx_i(X^TX)^{-1}}{1-h_{ii}} \\
                \end{align*}
            \item[b)]
                \begin{align*}
                    \hat \beta_{(i)} &= (X_{(i)}^TX_{(i)})^{-1}X_{(i)}^T Y_{(i)}   \\
                    \hat \beta_{(i)} &= \left((X^TX)^{-1} + \frac{(X^TX)^{-1}x_i^Tx_i(X^TX)^{-1}}{1-h_{ii}} \right)\left(X^TY - x_i^Ty_i\right)  \\
                    \hat \beta_{(i)} &= (X^TX)^{-1}\left(X^TY - x_i^Ty_i\right) + \frac{(X^TX)^{-1}x_i^Tx_i(X^TX)^{-1}}{1-h_{ii}}\left(X^TY - x_i^T y_i\right)   \\
                    \hat \beta_{(i)} &= \hat \beta - (X^TX)^{-1}x_i^Ty_i + \frac{(X^TX)^{-1}x_i^Tx_i(X^TX)^{-1}\left(X^TY - x_i^T y_i\right)}{1-h_{ii}}   \\
                    \hat \beta_{(i)} &= \hat \beta + \frac{(X^TX)^{-1}x_i^Ty_i(h_{ii}-1) +(X^TX)^{-1}x_i^T\left(x_i\hat\beta - h_{ii}y_i\right)}{1-h_{ii}}   \\
                    \hat \beta_{(i)} &= \hat \beta + \frac{(X^TX)^{-1}x_i^T\left(x_i\hat\beta - h_{ii}y_i + (1-h_{ii})y_i\right)}{1-h_{ii}}   \\
                    \hat \beta_{(i)} &= \hat \beta + \frac{(X^TX)^{-1}x_i^T\left(x_i\hat\beta - y_i\right)}{1-h_{ii}} \\
                    \hat \beta_{(i)} &= \hat \beta - \frac{(X^TX)^{-1}x_i^T\hat e_i}{1-h_{ii}}   
                \end{align*}
            \item[c)]
                \begin{align*}
                    y_i - x_i\hat\beta_{(i)} &= y_i- x_i\left(\hat \beta - \frac{(X^TX)^{-1}x_i^T\hat e_i}{1-h_{ii}}\right) \\
                    y_i - x_i\hat\beta_{(i)} &= y_i  - \hat y_i + \frac{h_{ii}\hat e_i}{1-h_{ii}} \\
                    y_i - x_i\hat\beta_{(i)} &= \hat e_i \left(1+ \frac{h_{ii}}{1-h_{ii}}\right) \\
                    y_i - x_i\hat\beta_{(i)} &= \hat e_i \frac{1}{1-h_{ii}} \\
                \end{align*}
            \item[d)]
                \begin{align*}
                    D_i &= \frac{\|\hat Y_{(i)} - \hat Y\|^2}{p\hat\sigma^2} \\
                    D_i &= \frac{\|X\beta_{(i)} - X\beta\|^2}{p\hat\sigma^2} \\
                    D_i &= \frac{\|X\left(\hat \beta - \frac{(X^TX)^{-1}x_i^T\hat e_i}{1-h_{ii}}\right) - X\beta\|^2}{p\hat\sigma^2} \\
                    D_i &= \frac{\|- X\frac{(X^TX)^{-1}x_i^T\hat e_i}{1-h_{ii}}\|^2}{p\hat\sigma^2} \\
                    D_i &= \frac{\frac{\hat e_ix_i(X^TX)^{-1}}{1-h_{ii}}X^TX\frac{(X^TX)^{-1}x_i^T\hat e_i}{1-h_{ii}}}{p\hat\sigma^2} \\
                    D_i &= \frac{\frac{\hat e_ix_i(X^TX)^{-1}}{1-h_{ii}}X^TX\frac{(X^TX)^{-1}x_i^T\hat e_i}{1-h_{ii}}}{p\hat\sigma^2} \\
                    D_i &= \frac{\frac{\hat e_ix_i(X^TX)^{-1}}{1-h_{ii}}\frac{x_i^T\hat e_i}{1-h_{ii}}}{p\hat\sigma^2} \\
                    D_i &= \frac{1}{p}\frac{\hat e_i^2}{\hat\sigma^2(1-h_{ii})}\frac{x_i(X^TX)^{-1}x_i^T}{1-h_{ii}} \\
                    D_i &= \frac{1}{p} r_i^2 \frac{h_ii}{1-h_{ii}} 
                \end{align*}
        \end{itemize}
    \item[3)]
        \begin{itemize}
            \item[a)]
                The two graphs make it pretty clear there is autocorrelated errors. The first, "Residual by Year", seems to show the errors moving up and down together, rather than being the cloud centered around $0$ if there was no autocorrelation. The second, "Residual vs Residual With Lag of 1" shows each residual plotted against the next residual. The evidence of autocorrelation is overwhelming here, with there being a clear positive linear relationship between one error and the next. \\
                \includegraphics[width=9cm]{hw6_3_a_res} 
                \includegraphics[width=9cm]{hw6_3_a_auto}  \\
            \item[b)]
                Running the AR1 model, we see an estimated autocorrelation of $.972$, which is extremely significant. We also see unemployed switch to being significant with a p-value of $.02$ in the AR1 model while just outside of of significance at $.051$ in the ordinary linear model. Much more importantly though, the coefficient switches sign, speaking to the autocorrelation having a meaningful effect.\\
                Confidence interval and estimate for autocorrelation: \\

                \input{hw6_3_alm_acor} 

                Coefficients for autocorrelated model: \\

                \input{hw6_3_alm_coef}

                Coefficients for regular linear model: \\
                \input{hw6_3_lm_coef}

                Only one coefficient, that of unemployed, switched to the other side of the $.05$ threshold with the auto correlated term, but the estimate and p-value of all the other variables changes noticeably, speaking to how not dealing with autocorrelation can invalidate a model.

            \item[c)]
                My suspicion is that the autocorrelation largely has to do with the relative size of different generations. The portion of the US population in different age groups varies a lot over time, and divorce rates are likely quite different between different age groups. Since the relative portion of people in each age group is highly auto-correlated (since everyone either dies or gets just one year older), our divorce rate is also auto-correlated.
        \end{itemize}
    \item[4)]
        It looks like we have a lack of fit since the variance isn't constant. Looking at the first plot, the residual plot, nothing is glaringly wrong, but it looks like the points farther to the right are somewhat more spread out. Plotting the square root transformed residuals against the predicted values, and adding a trend line, we see a much more obvious increase in variance over the range, with a clear upwards trend. Testing this formally with the variance test, where we split the residuals into those with speed above and below 14, we get a ratio of $.206$, with an associated p-value of $.0009$, which seals it. We also appear to have a mildly non-linear response. \\
        \includegraphics[width=9cm]{hw6_4_res} 
        \includegraphics[width=9cm]{hw6_4_sqrtres}  \\
             As far as other issues, none jump out. From the residual plot, no residual looks extreme enough to be an outlier. Running a QQ plot, the residuals on the whole look roughly normal. \\
        \begin{center}
            \includegraphics[width=9cm]{hw6_4_qq} 
        \end{center}
        The one potential issue is that the true relationship between the predictor and outcome may not quite be linear. Looking at the linear fit versus a kernel regression fit, it seems like the function may actual be somewhat convex. 
        \begin{center}
            \includegraphics[width=9cm]{hw6_4_ker} 
        \end{center}
        Attempting to approximate this with a linear spline, we allow for a different slope above and below a speed of $20$, where we seem to have a mild kink. When we do this, and run we ANOVA, we just get significant decrease in variance, with a p-value of $.0378$. \\
        \input{hw6_4}
    \item[5)]


    
\end{itemize}

\end{document}
