
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
STAT 343 Homework 6 \hfill Aaron Maurer
\vspace{2mm}
\hrule
\vspace{2mm}
\begin{itemize} 
    \item[1.]
        \begin{itemize} 
            \item[a)]
                When we run a linear regression, we see get a statistically significant positive coefficient on year. This at least is a pretty good indicator for some kind of increasing trend. However, looking at the plot, it looks like there is some kind of concavity at least during much of the 20th century, which doesn't support their being a simple linear trend. So, I would conclude there is an upwards trend that probably isn't quite linear.  
                \smallskip
                The estimate of coefficients:
                \FloatBarrier
                \input{hw7/hw7_1_a} 
                \FloatBarrier
                \begin{center}
                    \includegraphics[width=9cm]{hw7/hw7_1_a} 
                \end{center}
            \item[b)]
                When we run a linear model that includes an AR(1) term, we find a statistically significant autoregressive term, with an estimate autocorrelation of $.208$. However, even with this, the estimates for the normal coefficients are pretty much the same as with the simple linear regression, with the coefficient on year still at $.012$ and only a small change in the intercept. The plot also shows how the plot barely changed. So, the autocorrelation  doesn't change my opinion about the trend. 
                \smallskip
                The estimate of autocorrelation:
                \FloatBarrier
                \input{hw7/hw7_1_b_acor} 
                \FloatBarrier
                The estimate of coefficients:
                \FloatBarrier
                \input{hw7/hw7_1_b_ttable} 
                \FloatBarrier
                \begin{center}
                    \includegraphics[width=9cm]{hw7/hw7_1_b} 
                \end{center}

            \item[c)]
                Starting with orthogonal polynomials of degree one through ten and backwards eliminating them with a critical value of $.2$, I we end up left with polynomials of degree one through five. The coefficients and plot are below. Looking at the plot, the model seems to fit quite well within the range of the data, but when we predict out to $2020$, we get a prediction of $60.078$, which is way outside of the range seen historically and thus not believable. This reflects the issues with polynomials only being a good representation of arbitrary functions where there is data to fit them.

                \smallskip
                Predicted value and range:
                \FloatBarrier
                \input{hw7/hw7_1_c_pred} 
                \FloatBarrier               

                Estimated coefficients:
                \FloatBarrier
                \input{hw7/hw7_1_c} 
                \FloatBarrier
                \begin{center}
                    \includegraphics[width=9cm]{hw7/hw7_1_c} 
                \end{center}

            \item[d)]
                To test this trend, we fit a simple linear spline on the data, with an intercept and a continuous term for year which is $0$ before 1930. Looking at the results, we get a statistically significant coefficient on year after 1930. However, this coefficient is pretty close to the coefficient on year we had in the simple linear model, and the fit, when plotted, looks like a less convincing fit than the original simple linear model, with the actual values not symmetrically distributed around the prediction in the flat section. We don't have a formal statistical test to reject this model, but it certainly doesn't seem like an improvement over the simple linear model.

                \smallskip
                Estimated coefficients:
                \FloatBarrier
                \input{hw7/hw7_1_d} 
                \FloatBarrier
                \begin{center}
                    \includegraphics[width=9cm]{hw7/hw7_1_d} 
                \end{center}

            \item[e)]
                Fitting a cubic spline model, we certainly seem to see an improvement over the simple linear model. It picks up the apparent concavity over most of the 20th century, and also seems to capture the behavior towards the boundary reasonably, besides possibly turning up too sharply at the high end of the domain. Its fit is fairly similar to the polynomial fit though, with the exception of not having an inflection point on the lower end of the domain. There isn't much data in that section though, so it looks more like the polynomial is over fitting than the cubic splines.

                \smallskip
                Estimated coefficients:
                \FloatBarrier
                \input{hw7/hw7_1_e} 
                \FloatBarrier
                \begin{center}
                    \includegraphics[width=9cm]{hw7/hw7_1_e} 
                \end{center}
        \end{itemize}
    \item[2.]
    \item[4.]
        As I think we had to show on the midterm,
        \begin{align*}
            H&= X(X^TX)^{-1}X^T \\
            H&= QR(R^TQ^TQR)^{-1}R^TQ^T \\
            H&= QR(R^TR)^{-1}R^TQ^T \\
            H&= QRR^{-1}(R^T)^{-1}R^TQ^T \\
            H&= QQ^T
        \end{align*}
        As was discussed with added variable plots, for an additional variable $X_i$ added to a regression, if $e_{(-i)}$ are the residuals from the regression excluding $X_i$ and $e$ are the residuals from the regression including it,
        \begin{align*}
            e_{(-i)} &= \hat\beta_i (I-H) X_i + e \\ 
            e_{(-i)} &= \hat\beta_i (I-QQ^T) X_i - e \\ 
        \end{align*}
        From simple linear regression, since \(e_{(-i)}\) has mean $0$, it must be the case that \(\hat\beta_i=\frac{\cov((I-QQ^T) X_i,e_{(-i)})}{\var((I-QQ^T) X_i)}\). Plugging this in, we get
        \begin{align*}
            e_{(-i)} &= \frac{\cov((I-QQ^T) X_i,e_{(-i)})}{\var((I-QQ^T) X_i)} (I-QQ^T) X_i - e \\ 
            e &= e_{(-i)} - \frac{\cov\left((I-QQ^T) X_i,e_{(-i)}\right)}{\var\left((I-QQ^T) X_i\right)} (I-QQ^T) X_i \\ 
            \|e\|^2 &= \left\|e_{(-i)} - \frac{\cov\left((I-QQ^T) X_i,e_{(-i)}\right)}{\var\left((I-QQ^T) X_i\right)} (I-QQ^T) X_i \right \|^2
            RSS &= \left\|e_{(-i)} - \frac{\cov\left((I-QQ^T) X_i,e_{(-i)}\right)}{\var\left((I-QQ^T) X_i\right)} (I-QQ^T) X_i \right \|^2
        \end{align*}
        Thus, 
 

\end{itemize}
\end{document}
