
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%% AMS PACKAGES - Chances are you will want some or all of these if writing a math dissertation.
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, enumerate, multicol, graphicx, listings}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Homework 3 \hfill Aaron Maurer
\vspace{2mm}
\hrule
\vspace{2mm}

\begin{itemize}
    \item[1.]
        \begin{itemize}
            \item[a)]
                The columns of $X$ are linearly independent, so \(rank(X)=p\). This in turn dictates that \(rank(QR)=p\). Since the rank of the product of two matrices can be no greater than the smallest rank of the factors, we must conclude that \(rank(Q)=rank(R)=p\), since they both can't have greater rank than $p$. Since $R$ has full rank, it must be invertible.
            \item[b)]
                Multiplying both sides of the linear system through by $Q$, we get that
                \begin{align*}
                    R \tilde\beta &= Q^TY \\
                    QR \tilde\beta &= QQ^TY \\
                    X\tilde\beta &= Y \\
                \end{align*}
                So we may deduce that $Y$ is in the span of $X$. Thus, for $H=X(X^TX)^{-1}X^T$, $HY=Y$, since $HZ=Z$ for all vectors $Z$ in the span of $X$. Starting with this, we get that
                \begin{align*}
                    HY &=Y \\
                    X(X^TX)^{-1}X^TY &= Y \\
                    X \hat\beta &= Y
                \end{align*}
                Since, $\tilde\beta$ is unique, we may conclude that $\tilde\beta=\hat\beta$.
            \item[c)]
                The "normal equation" to calculate $\hat\beta$ ran into a singularity at $\epsilon=1e-5$, while the QR method of calculation still worked. The R code used for this calculation is attached in the back.
            \item[d)]
                The QR method and the linear model returned the following results for $\epsilon=1e-5$: \\
                \begin{tabular}{l | l l}
                    Coefficient & QR Method & Linear Model  \\
                    \hline
                    Intercept           &  2.862e+1    &   2.862e+1\\
                    pop15               & -4.620e-1    &  -4.620e-1\\
                    pop75               & -1.151e+4    &  -1.151e+4\\
                    dpi                 & -3.356e-4    &  -3.356e-4\\
                    ddpi                &  4.088e-1    &   4.088e-1\\
                    pop75 + $u\epsilon$ &  1.151e+4    &   1.151e+4
                \end{tabular} \\
                It is not surprising that they returned the same estimates, since we were told that the linear model in R is using the QR method of calculation. The one surprising result is that the two pop75 variables had such big coefficients, but this is due to them being highly collinear, which is why their coefficients added together are almost $0$.  
        \end{itemize}
    \item[2.]
        %Since $A$ and $B$ are both positive semidefinite and symmetric, we can decompose both using Cholesky decomposition. Let $K$ and $L$ be lower triangular matrices such $A=KK^T$ and $B=LL^T$. This gives us $Y^TKK^TY=\|Y^TK\|^2$ and $Y^TLL^TY=\|L^TY\|^2$. These two are independent if $L^TY$ and $Y^TK$ are independent. Since each of these variables is a linear combination of the random vector $Y$, which has a multivariate normal distribution, these two sets of variables are pairwise jointly normally distributed. Thus, all it takes to prove normality is that their covariance is $0$. 
        %\[ \cov(Y^TK \]
       
        $AY$ and $BY$ are jointly normally distributed, since they are both linear combinations of $Y$, which is drawn from a multivariate normal distribution. We can also show that their covariance is $0$:
        \begin{align*}
            \cov[AY,BY]&=\E[AY(BY)^T] \\
                       &=\E[AYY^TB^T] \\
                       &=\E[\|Y\|^2AB] \\
                       &=\E[0] \\
                       &=0
        \end{align*}
        Since $AY$ and $BY$ are jointly normally distributed, this is sufficient to prove their independence. Now, since $YAY$ and $YBY$ are merely linear combinations of the variables in $AY$ and $BY$, which are pairwise independent, $YAY$ and $YBY$ must also be linearly independent.

    \item[3.]
        Since $A$ is symmetric, we can eigendecompose it, so that $A=PDP^T$, where $D$ is a diagonal matrix of $A$'s eigenvalues, and $P$ is an orthogonal matrix of $A$'s eigenvectors. It is the case that, where $L$ is the set of all eigenvalues of $A$:
        \[ p=tr(A)=tr(D)=\sum_{\lambda\in L} \lambda \]
        Since all of $A$'s eigenvalues are either $0$ or $1$, this means that $p$ is the number of non-zero eigenvalues of $A$. Also,
        \begin{align*}
            e^TAe &= tr(e^TAe) \\
                  &= tr(e^TPDP^Te) \\
                  &= tr(e^TDP^TPe) \\
                  &= tr(e^TDe) \\
                  &= e^TDe  \\
                  &= \sum_{i=1}^n e_i^2 I_{\{j:d_{jj}=1\}}(i) \\
        \end{align*}
        The last line is based on the fact that the eigen values of $A$, which are also the diagonal of $D$, are all either $0$ or $1$. Clearly, we will have $p$ terms in our summand, since that is how many eigenvalues of $1$ there are. Since each $e_i$ is independent of all other $e_j$, this means $e^TAe$ is the sum of $p$ independent squared normals, or in other words
        \[ e^TAe \sim \chi_p^2\]

    \item[4.]
        \begin{align*}
            \cov(\hat e, \hat Y) &= \E\left[\hat e \hat Y^T\right] - \E\left[\hat e\right] \E\left[\hat Y\right]^T & \\
                                 &= \E\left[0_{n\times n}\right] - 0_{n \times 1} Y^T          & \mbox{Since } \hat e \mbox{ and } \hat Y \mbox{are orthogonal} \\
                                 &= 0_{n \times n}
        \end{align*}
    \item[5.]
        \begin{itemize}
            \item[3.1]
                \begin{itemize}
                    \item[a)]
                        After running the model and getting the coefficient for age and its standard error, the confidence intervals are:
                        \[ 90\% \mbox{ CI} = \beta_{age} \pm SE \cdot  t_{rdf}^{.05} = -.0196 \pm .0112 \cdot 1.6624 = (-.0382,-.0012) \]
                        \[ 95\% \mbox{ CI} = \beta_{age} \pm SE \cdot  t_{rdf}^{.025} = -.0196 \pm .0112 \cdot 1.9873 = (-.0418,0026) \]
                        Since the $90\%$ CI doesn't include $0$, but the $95\%$ CI does, we can conclude the pvalue for $\beta_{age}$ is between $.05$ and $.1$.
                    \item[c)]
                        The simulation, the code for which is in my code supplement, run on $1000$ permutations of lpsa, had $8.8\%$ of the results with a higher t value for $\beta_{age}$.
                    \item[d)]
                        Running the F test with the null hypothesis that a model just with lcavol, lweight, and svi is the real model, one would expect to see at least as large an improvement from adding four variables as we saw by adding lbph, age, lcp, and gleason $21.67\%$ of the time. Thus , we can not reject the null hypothesis, and prefer the simpler model.
                \end{itemize}
            \item[3.2]
                \begin{itemize}
                    \item[a)]
                        Running a linear model predicting taste in terms of the other three variables, only H2S and Lactic are statistically significant. 
                    \item[b)]
                        Transforming Acetic and H2S back from the log scale, H2S ceases to be statistically significant, Acetic remains insignificant, and Lactic remains significant. 
                \end{itemize}
                
        \end{itemize}
\end{itemize}

\newpage
\hrule
\vspace{2mm}
R Code Supplement
\vspace{2mm}
\hrule
\lstinputlisting{hw3.R}

\end{document}
