
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins, bigints} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\p}{\mathrm{P}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newcommand{\hwhead}[1]{#1 \hfill Aaron Maurer \vspace{2mm} \hrule \vspace{2mm}}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hwhead{STAT 302 Homework 4}
\begin{itemize}
    \item[1.]
        \begin{itemize}
            \item[i)]
                Together, (1) and (2) define a hierarchical model since they relate observed values $D_{ij}$ drawn independently based on parameters $\beta_i$ to a set of hyperparameters $\pi_0$ and $\sigma_b$ in such a way that the $D_{ij}$ can provide inference on each other through the hyperparameters. In other words, even though the observations are conditionally independent, they are still informative on each other through the hyperparameters.Since the $\beta_j$ are iid given the hyperparameters, they are necessarily exchangeable. Not conditioned on $\pi_0, \sigma_b$, the $\beta_j$ should still be exchangeable assuming the same value of $\pi_0$ and $\sigma_b$ are used to draw each $\beta_j$; there is just additional variability in the initial draw of the hyperparameters. $D_{ij}\st \beta, \sigma$ will be exchangeable for all $i$ since they are iid, but $D_{ij}\st \beta_j$ won't be exchangeable across $j$ since \(P(D_{ij}\st \beta_j=a,D_{ij'}\st \beta_{j'}=b)=P(D_{ij'}\st \beta_{j'}=a,D_{ij}\st \beta_j=b)$ only if $\beta_{j'}=\beta_j$. Finally, the $D_{ij}$ should be exchangable accross $i$ and $j$ if unconditioned, or conditioned on $\pi,\sigma_b$, since then they will be iid.
            \item[ii)]
                This command will calculate $\beta$, $D$, and also the p-values.
                \lstinputlisting[firstline=18,lastline=28]{hw4.R}
            \item[iii)] I calculated the usual two sided t-test p-value using the R command. They were distributed as such: \\
                \includegraphics[width=9cm]{hw4/pr1_iii_hist_a}
                \includegraphics[width=9cm]{hw4/pr1_iii_hist_b} \\
                \includegraphics[width=9cm]{hw4/pr1_iii_hist_c} \\
                As one would expect, for the first one where the $\beta$ are all $0$, the p-values are uniformly distributed. In the second, with $\pi_0=.5$, we see a big lump of low p-values near $0$ corresponding to most of the $\beta_j$ which are non-zero, and a approximately uniform distribution over the rest of the p-values corresponding to the $\beta_j$ which are $0$. Finally, in the last histogram, where none of the $\beta_j$ are $0$, we see overwhelmingly low p-values corresponding to the strong evidence against $\beta_j=0$.
            \item[iv)] This is my function to apply Benjamini-Hochberg:
                \lstinputlisting[firstline=44,lastline=50]{hw4.R}
            \item[v)] This is my function to computer the empirical FDR (and pFDR):
                \lstinputlisting[firstline=53,lastline=58]{hw4.R}
            \item[vi)]
                To simulate $\E(V/R)$, I generated the dataset D $1000$ times for each scenario, and averaged the emperical FDR for each $alpha$ over the data sets. The results are ploted below: \\
                \includegraphics[width=9cm]{hw4/pr1_vi_a}
                \includegraphics[width=9cm]{hw4/pr1_vi_b} \\
                \includegraphics[width=9cm]{hw4/pr1_vi_c} \\
                In the first scenario, despite the fact that all the discoveries are false, our FDR statistic is less than 1, which is due to the definiton. In any event, The FDR rate is properly controlled, coming in under $\alpha$. For the second scenario, the FDR rate seems to come in at almost exactly half of $\alpha$ for all levels, providing conservative control. For the final scenario, since all $\beta\neq0$, the FDR is necessarily $0$, so FDR is inherently controlled.  
            \item[vii)] I repeated the same procedure as above, but this time caclulated the pFDR. The results are below:
                \includegraphics[width=9cm]{hw4/pr1_vii_a}
                \includegraphics[width=9cm]{hw4/pr1_vii_b} \\
                \includegraphics[width=9cm]{hw4/pr1_vii_c} \\
                In the first case, with all discoveries false, the pFDR is naturally at 1 for all $\alpha$ and not controlled. The second case is nearly identical as with FDR, controlled at about half of $\alpha$. Again, for the final case, since no discoveries are false, the pFDR is controlled perfectly at 0.
        \end{itemize}
    \item[2.]
        \begin{itemize}
            \item[i)]        
                Repeating the same procedure as above, except with reject decisions based on q values, I get these results: \\
                \includegraphics[width=9cm]{hw4/pr2_i_FDR_a}
                \includegraphics[width=9cm]{hw4/pr2_i_pFDR_a} \\
                In this first scenario, the FDR seems to be controlled at pretty much exactly the desired rate, while the pFDR is still necessarily 1.\\
                \includegraphics[width=9cm]{hw4/pr2_i_FDR_b}
                \includegraphics[width=9cm]{hw4/pr2_i_pFDR_b} \\
                In this second scenario, we have control of both FDR and pFDR at the desired rate.\\
                \includegraphics[width=9cm]{hw4/pr2_i_FDR_c}
                \includegraphics[width=9cm]{hw4/pr2_i_pFDR_c} \\
                With no false discoveries, FDR and pFDR are still 0.
            \item[2)]
                I have plotted the estimated $\pi_0$ vs actual $\pi_0$ over a range of values for $\sigma_b=.5,1,2,4$. Each estimate is the average over 100 simulations. \\
                \includegraphics[width=9cm]{hw4/pr2_ii_pt5}
                \includegraphics[width=9cm]{hw4/pr2_ii_1}\\
                As we can see above and below, the estimates get increasingly good as the variance goes up and the $\beta$ tend to fall farther from $0$. The predictions are uniformly higher than the actual $\pi_0$, except for values near 1, and the worst predictions are for low values of $\pi_0$. In general, this method seem to provide a pretty solid estimate. \\
                \includegraphics[width=9cm]{hw4/pr2_ii_2}
                \includegraphics[width=9cm]{hw4/pr2_ii_4}\\
        \end{itemize}
    \item[3.]
        \begin{itemize}
            \item[i)]
                We can prove this by using the conditional independence of the $D_ij$ and decomposing the likelihood with $\sigma$ fixed:
                \begin{align*}
                    \p(D\st\beta) &= \prod_{j=1}^m \prod_{i=1}^n \p(D_{ij}\st\beta_j) \\
                    \p(D\st\beta) &\propto \prod_{j=1}^m \prod_{i=1}^n \exp\left\{-\frac{1}{2\sigma^2}(\beta_j^2-2\beta_jD_{ij})\right\} \\
                    \p(D\st\beta) &\propto \prod_{j=1}^m \exp\left\{-\frac{n}{2\sigma^2}\left(\beta_j^2-2\beta_j\sum_{j=1}^n\frac{D_{ij}}{n}\right)\right\} \\
                    \p(D\st\beta) &\propto \prod_{j=1}^m \exp\left\{-\frac{n}{2\sigma^2}\left(\beta_j^2-2\beta_j\bar D_{j}\right)\right\} \\
                    \p(D\st\beta) &\propto \prod_{j=1}^m \exp\left\{-\frac{1}{2\frac{\sigma^2}{n}}\left(\beta_j^2-2\beta_j\bar D_{j}\right)\right\} \\
                    \p(D\st\beta) &\propto \prod_{j=1}^m \p(\bar D_j \st \beta_j) \\
                    \p(D\st\beta) &\propto \p(\bar D \st \beta) \\
                \end{align*}
                Proving that $\bar D$ is a sufficient statistic for the full matrix $D$.
            \item[ii)]
                We can derive the log-likelihood by first integrating out $\beta$ to get the pdf of $\bar D$, and then taking the log. Let $\sigma_n^2 = \frac{\sigma^2}{n}$.
                \begin{align*}
                    \p(\bar D\st \pi_0,\sigma_b) &= \prod_{j=1}^m \p(\bar D_j\st \pi_0,\sigma_b)\\
                    \p(\bar D\st \pi_0,\sigma_b) &= \prod_{j=1}^m\int_\R \p(\bar D_j\st \beta_j) d\p_{\beta_j \st \pi_0,\sigma_b}\\
                    \p(\bar D\st \pi_0,\sigma_b) &= \prod_{j=1}^m\left(\pi_0\p(\bar D_j\st \beta_j=0) + \frac{1-\pi_0}{\sigma_b\sqrt{2\pi}}\bigintsss_\R \p(\bar D_j\st \beta_j)\exp\left\{-\frac{\beta_j^2}{2\sigma_b^2}\right\}d\beta_j\right) \\
                    \p(\bar D\st \pi_0,\sigma_b) &\propto \prod_{j=1}^m\left(\pi_0 + \frac{1-\pi_0}{\sigma_b\sqrt{2\pi}}\bigintsss_\R \exp\left\{-\frac{1}{2\sigma_n^2}\left(\beta_j^2-2\beta_j\bar D_{j}\right)\right\}\exp\left\{-\frac{\beta_j^2}{2\sigma_b^2}\right\}d\beta_j\right) \\
                    \p(\bar D\st \pi_0,\sigma_b) &\propto \prod_{j=1}^m\left(\pi_0 + \frac{1-\pi_0}{\sigma_b\sqrt{2\pi}}\bigintsss_\R \exp\left\{-\frac{\sigma_n^2+\sigma_b^2}{2\sigma_n^2\sigma_b^2}\left(\beta_j^2-2\frac{\sigma_b^2}{\sigma_n^2+\sigma_b^2}\beta_j\bar D_{j}\right)\right\}d\beta_j\right)\\
                    \p(\bar D\st \pi_0,\sigma_b) &\propto \prod_{j=1}^m\left(\pi_0 + \frac{1-\pi_0}{\sigma_b\sqrt{2\pi}}\exp\left\{\frac{\sigma_b^2}{2\sigma_n^2(\sigma_n^2+\sigma_b^2)}\bar D_j^2\right\}          \bigintsss_\R \exp\left\{-\frac{\sigma_n^2+\sigma_b^2}{2\sigma_n^2\sigma_b^2}\left(\beta_j-\frac{\sigma_b^2}{\sigma_n^2+\sigma_b^2}\bar D_{j}\right)^2\right\}d\beta_j\right)\\
                    \p(\bar D\st \pi_0,\sigma_b) &\propto \prod_{j=1}^m\left(\pi_0 + \frac{1-\pi_0}{\sigma_b\sqrt{2\pi}}\exp\left\{\frac{\sigma_b^2}{2\sigma_n^2(\sigma_n^2+\sigma_b^2)}\bar D_j^2\right\}  \sqrt{\frac{2\pi\sigma_n^2\sigma_b^2}{\sigma_n^2+\sigma_b^2}}\right) \\
                    \p(\bar D\st \pi_0,\sigma_b) &\propto \prod_{j=1}^m\left(\pi_0 + \frac{(1-\pi_0)\sigma_n}{\sqrt{\sigma_n^2+\sigma_b^2}}\exp\left\{\frac{\sigma_b^2}{2\sigma_n^2(\sigma_n^2+\sigma_b^2)}\bar D_j^2\right\}  \right) \\
                    l(\pi_0,\sigma_b) &= c +  \sum_{j=1}^m\log\left(\pi_0 + \frac{(1-\pi_0)\sigma_n}{\sqrt{\sigma_n^2+\sigma_b^2}}\exp\left\{\frac{\sigma_b^2}{2\sigma_n^2(\sigma_n^2+\sigma_b^2)}\bar D_j^2\right\}  \right) \\
                \end{align*}
            \item[iii)]
                I've implemented a method that maximizes the likelihood given above. In testing, it converged fine, but there must be some mistake in my derivation or my code I can't figure out, since it yields consistantly the wrong answer. None the less, this is my code:
                \lstinputlisting[firstline=186,lastline=218]{hw4.R}
            \item[iv)]
                Using the expression for likelihood we derived at the second to last step of ii, we can calculate \\ $\p(\beta_j=0\st \bar D_j, \pi_0, \sigma_b)$ as 
                \begin{align*}
                    \p(\beta_j=0\st \bar D_j, \pi_0, \sigma_b) &= \frac{\p(\beta_j=0\st \pi_0, \sigma_b)\p(\bar D_j \st \beta_j=0)}{\p( \bar D_j \st \pi_0, \sigma_b)} \\
                    \p(\beta_j=0\st \bar D_j, \pi_0, \sigma_b) &= \frac{\pi_0}{ \pi_0 + \frac{(1-\pi_0)\sigma_n}{\sqrt{\sigma_n^2+\sigma_b^2}}\exp\left\{\frac{\sigma_b^2}{2\sigma_n^2(\sigma_n^2+\sigma_b^2)}\bar D_j^2\right\} } \\
                    \p(\beta_j=0\st \bar D_j, \pi_0, \sigma_b) &:= \pi_1 \\
                \end{align*}
                We can then derive $\p(\beta_j\st \bar D_j, \pi_0, \sigma_b,\beta_j\neq0)$ as 
                \begin{align*}
                    \p(\beta_j\st \bar D_j, \pi_0, \sigma_b, \beta_j\neq 0) &\propto  \p(\bar D_j\st \beta_j) \p(\beta_j \st \pi_0, \sigma_b, \beta_j\neq 0) \\
                    \p(\beta_j\st \bar D_j, \pi_0, \sigma_b, \beta_j\neq 0) &\propto  \exp\left\{-\frac{1}{2\sigma_n^2}\left(\beta_j^2-2\beta_j\bar D_j\right) -\frac{1}{2\sigma_b^2}\beta_j^2\right\} \\
                    \p(\beta_j\st \bar D_j, \pi_0, \sigma_b, \beta_j\neq 0) &\propto  \exp\left\{-\frac{\sigma_n^2+\sigma_b^2}{2\sigma_n^2\sigma_b^2}\left(\beta_j^2-2\frac{\sigma_b^2}{\sigma_n^2+\sigma_b^2}\bar \beta_jD_{j}\right)\right\} \\
                    \p(\beta_j\st \bar D, \pi_0, \sigma_b, \beta_j\neq 0) &= \frac{1}{\sqrt{\frac{\sigma_n^2\sigma_b^2}{\sigma_n^2+\sigma_b^2}2\pi}} \exp\left\{-\frac{\sigma_n^2+\sigma_b^2}{2\sigma_n^2\sigma_b^2}\left(\beta_j-\frac{\sigma_b^2}{\sigma_n^2+\sigma_b^2}\bar D_{j}\right)^2\right\} \\
                    \p(\beta_j\st \bar D, \pi_0, \sigma_b, \beta_j\neq 0) &= \sqrt{\frac{1}{\sigma_b^2}+\frac{1}{\sigma_n^2}} \frac{1}{2\pi} \exp\left\{-\left(\frac{1}{\sigma_b^2}+\frac{1}{\sigma_n^2}\right)\left(\beta_j-\frac{\sigma_b^2}{\sigma_n^2+\sigma_b^2}\bar D_{j}\right)^2\right\}
                \end{align*}
                Giving us a total estimate of 
                \[ \p(\beta_j\st\bar D, \pi_0, \sigma_b) = \begin{cases} 
                        (1-\pi_1)\sqrt{\frac{1}{\sigma_b^2}+\frac{1}{\sigma_n^2}} \frac{1}{2\pi} \exp\left\{-\left(\frac{1}{\sigma_b^2}+\frac{1}{\sigma_n^2}\right)\left(\beta_j-\frac{\sigma_b^2}{\sigma_n^2+\sigma_b^2}\bar D_{j}\right)^2\right\} &\mbox{if } \beta_j\neq 0 \\
                        \pi_1 &\mbox{if } \beta_j=0 \end{cases} \]
            \item[v)] Since my likelihood is screwed up, I didn't actually use the optimized parameters and cheated. However, I still use the likelihood in part of my calculation of p-values as stated above, so the end result seems to still be that something is off. Non withstanding, here are my results: \\
                \includegraphics[width=9cm]{hw4/pr3_v_FDR_a}
                \includegraphics[width=9cm]{hw4/pr3_v_pFDR_a} \\
                \includegraphics[width=9cm]{hw4/pr3_v_FDR_b}
                \includegraphics[width=9cm]{hw4/pr3_v_pFDR_b} \\
                \includegraphics[width=9cm]{hw4/pr3_v_FDR_c}
                \includegraphics[width=9cm]{hw4/pr3_v_pFDR_c} \\
                I think my mistake made my test way overly conservative in a way I don't quite understand.
        \end{itemize}
\end{itemize}
\end{document}
