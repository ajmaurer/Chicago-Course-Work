
\documentclass[11pt]{article}
\usepackage[paper=letterpaper, margin=.5in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength\parindent{0in}

%%% Packages
% First four - AMS (american mathematical society). General math goodness. I use the align* enviorment in particular
% multirow, multicol allow for certain kinds of tables
% enumerate lets you determine the style of the counter for the enumerate enviorment
% graphicx lets you include pictures
% listings lets you stick in blocks of code
% placeins defines "\FloatBarrier", which stops tables from moving around
\usepackage{amsmath, amscd, amssymb, amsthm, multirow, multicol, enumerate, graphicx, listings, placeins} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\del}{\partial}
\newcommand{\real}{\textrm{Re }}
\newcommand{\imag}{\textrm{Im }}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\sumk}{\sum_{k=1}^\infty}
\newcommand{\sumj}{\sum_{j=1}^\infty}
\newcommand{\sumn}{\sum_{n=0}^\infty}
\newcommand{\summ}[2]{\sum_{k=#1}^{#2}}
\newcommand{\sig}[1]{\sum_{#1 =1}^\infty}
\newcommand{\un}[1]{\bigcup_{#1 =1}^\infty}
\newcommand{\inter}[1]{\bigcap_{#1 =1}^\infty}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\ipxu}{\langle x,u_j \rangle}
\newcommand{\uj}{\{u_j\}_{j=1}^\infty}
\newcommand{\B}{\mathcal{B}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ST}{mbox{ s.t. }}

\newcommand{\Example}{\noindent {\bf Example. \quad} }
\newcommand{\Proof}{\noindent {\bf Proof: \quad} }
\newcommand{\Remark}{\noindent {\bf Remark. \quad} }
\newcommand{\Remarks}{\noindent {\bf Remarks. \quad} }
\newcommand{\Case}{\noindent {\underline{Case} \quad} }

\newcommand{\st}{ \; \big | \:}

\newcommand{\deuc}{d_{\mathrm euc}}
\newcommand{\dtaxi}{d_{\mathrm taxi}}
\newcommand{\ddisc}{d_{\mathrm disc}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
STAT 302 Homework 1 \hfill Aaron Maurer
\vspace{2mm}
\hrule
\vspace{2mm}

\begin{itemize}
    \item[1.]
        \begin{itemize}
            \item[(a)]
                We can calculate the type I error as
                \[P(x\in\{1,2\}|\theta=0) = P(x=1|\theta=0) + P(x=2|\theta=0) = .01\]
                And the type II error as 
                \[P(x\not\in\{1,2\}|\theta=1) = P(x=3|\theta=1) =.01\]
            \item[(b)]
                We can calculate the p-value for $x=1$ as 
                \[ \sum_{x: P(X=x|H_0)\leq P(X=1|H_0)} \hspace{-4em} P(X=x|H_0) = P(X=1|H_0) + P(X=2|H_0) = .01\]
                The Bayes Factor for $H_1$ vs. $H_2$ is
                \[\frac{P(x=1|\theta=1)}{P(x=1|\theta=0)} =\frac{.0049}{.005} = .98\]
                Qualitatively, this means that $x=1$ is very, very slightly evidence for $H_0$ over $H_1$.
            \item[(c)]
                We get a p-value for $x=2$ of 
                \[ \sum_{x: P(X=x|H_0)\leq P(X=2|H_0)} \hspace{-4em} P(X=x|H_0) = P(X=1|H_0) + P(X=2|H_0) = .01\]
                But this time the Bayes Factor $H_1$ vs. $H_2$ is
                \[\frac{P(x=2|\theta=1)}{P(x=2|\theta=0)} =\frac{.9851}{.005} = 197.02\]
                Which is overwhelming evidence for $H_1$
            \item[(d)]
                If we modify our example to \\
                \begin{table}[h!]
                \center
                \begin{tabular}{c|c|c|c}
                $x$ & 1 & 2 & 3 \\ \hline
                $p(x|\theta=0)$ & 0.005 & 0.005 & 0.99 \\
                $p(x|\theta=1)$ & 0.495 & 0.495 & 0.01 
                \end{tabular}
                \end{table}

                Then the p-value for $x=1$ is still .01, but now the Bayes Factor is
                \[\frac{P(x=1|\theta=1)}{P(x=1|\theta=0)} =\frac{.495}{.005} = 99\]
                Which is a very large and in favor of $H_1$
        \end{itemize}
    \item[2.]
        I simulated, for one sided p-values for the coefficient from a simple linear regression, how often a value between .04 and .05 actually corresponded to a true positive. Each p-value was simulated where the true effect $\beta$ was 0 with probability $1-\pi$ and drawn from an exponential with coefficient $\lambda$ with probability $\pi$. This was preformed with all the usual assumptions for a linear regression in place. To get each p-value, I simulated as if 10 units received the treatment, 10 received the control, and that underlying variance was 1. This was done by drawing \(\hat\beta\sim N(\beta,\frac{1}{\sqrt{10}})\), drawing \(\hat s^2 \sim \frac{\chi^2_{18}}{18}\), and generating the usual p-value from the resulting T score. \\
        \vspace{0mm} \\
        For a number of combinations of $\lambda$ and $\pi$, I simulated $100,000$ such p-values, and calculated the portion of such values between .04 and .05 for which the true $\beta$ was positive. I've include the results below (remember \(\E(\beta)=\frac{1}{\lambda}\)):
        \FloatBarrier
        \input{hw1/pr2.tex}
        \FloatBarrier
        Though I employ a somewhat different simulation, the main result is the Same as Selke et al; a p-value near .05 is not consistently strong evidence for the alternative hypothesis. Indeed, we also see that it provides the best evidence for the alternative with moderate levels of power, since if power is too high one rarely sees p-values near .05 for true positives. \\

        {\bf Code:}
        \lstinputlisting[firstline=22,lastline=50]{hw1.R}
    
    \item[3.]
        To get the posterior, we do the usual Bayesian calculation:
        \begin{align*}
            P(\mu|x) &\propto P(\mu) P(x_1,...,x_n|\mu) \\
                     &\propto \mu^{m-1}e^{-\lambda \mu} \prod_{i=1}^n \frac{\mu^{x_i}}{x_i!}e^{-\mu} \\
                     &\propto \mu^{m-1+\sum_{i=1}^n x_i}e^{-(\lambda+n) \mu} \prod_{i=1}^n \frac{1}{x_i!} \\
                     &\sim Gamma(m+\sum_{i=1}^n x_i,\lambda+n)
        \end{align*}

    \item[4.]
        To get the posterior, we do the usual Bayesian calculation, this time substituting in \(\tau=\frac{1}{\sigma^2}\):
        \begin{align*}
            P(\tau|x) &\propto P(\tau) P(x|\tau) \\
                      &\propto e^{-2\tau} \sqrt{\tau}e^{-\frac{1}{2}\tau x^2} \\
                      &\propto e^{-2\tau} \sqrt{\tau}e^{-\frac{1}{2}\tau x^2} \\
                      &\propto \tau^\frac{1}{2}e^{-(2+\frac{1}{2}x^2)\tau} \\
                      &\sim Gamma\left(\frac{3}{2},2+\frac{1}{2}x^2\right) 
        \end{align*}

    \item[5.]
        Overall, using this method, I had an error rate of $22.3\%$. The main issue I ran into with the method as described was that a number of allele/population combinations didn't appear in the training set. This isn't necessarily a problem, but with these 0 likelihoods included, some individuals in the test data set had 0 likelihoods of each population. I remedied this by adding one to each of the allele/locus counts in the test data set before calculating frequencies. This corresponds to beginning with a weak uniform prior on the allele/locus frequencies. Also, when an individual was missing allele data at a particular locus, I omitted that locus from likelihood calculation (in the program I just multiplied each likelihood by 1).
        
        {\bf Code:}
        \lstinputlisting[firstline=60,lastline=108]{hw1.R}

\end{itemize}

\end{document}
